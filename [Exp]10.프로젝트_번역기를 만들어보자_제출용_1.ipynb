{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d69d6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ee826",
   "metadata": {},
   "source": [
    "실습에서 구현한 번역기는 글자 단위(Character-level)에서 구현된 번역기였습니다. 하지만 실제 번역기의 경우에는 글자 단위가 아니라 단어 단위(Word-level)에서 구현되는 것이 좀 더 보편적입니다.\n",
    "\n",
    "동일한 데이터셋을 사용하면서 글자 단위와는 다른 전처리와 to_categorical() 함수가 아닌 임베딩 층(Embedding layer)를 추가하여 단어 단위의 번역기를 완성시켜보겠습니다. 하지만, 단어 단위로 할 경우에는 단어의 개수가 글자 단위로 했을 경우와 비교하여 단어장의 크기(Vocabulary) 크기도 커지고, 학습 속도도 좀 더 느려집니다. 학습과 테스트 시의 원활한 진행을 위해서 데이터에서 상위 33,000개의 샘플만 사용해주세요.\n",
    "\n",
    "33000개 중 3000개는 테스트 데이터로 분리하여 모델을 학습한 후에 번역을 테스트 하는 용도로 사용합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4331c",
   "metadata": {},
   "source": [
    "## Step 1. 정제, 정규화, 전처리 (영어, 프랑스어 모두!)\n",
    "\n",
    "글자 단위가 아닌 단어 단위의 번역기를 하기 위해서는 글자 단위에서는 신경쓰지 않았던 몇 가지 추가적인 전처리가 필요합니다.\n",
    "\n",
    "#### 1. 구두점(Punctuation)을 단어와 분리해주세요.\n",
    "\n",
    "일반적으로 영어권 언어의 경우에는 띄어쓰기 단위로 단어를 분리합니다. 토큰화(Tokenization) 라고도 불리는 이 작업은 어디서부터 어디까지가 하나의 단어인지를 구분하는 작업인데요, 그런데 띄어쓰기를 해주기 전에 구두점을 분리하는 작업이 필요할 때가 있습니다.\n",
    "예를 들어서 'he is a good boy!'라는 문장이 있을 때, 이를 띄어쓰기 단위로 토큰화한다면 ['he', 'is', 'a', 'good', 'boy!']가 됩니다. 그런데 실제로 !는 boy와 붙어있는 한 단어가 아니므로 좀 더 올바른 전처리는 ['he', 'is', 'a', 'good', 'boy', '!']가 맞습니다.\n",
    "!나 ? 또는 온점과 같은 특수문자들을 구두점(punctuation)이라고 부릅니다. 이들을 토큰화하기 전에 단어와 미리 분리시켜주세요!\n",
    "\n",
    "분리 전 : he is a Good boy!\n",
    "\n",
    "분리 후 : he is a Good boy !\n",
    "\n",
    "#### 2. 소문자로 바꿔주세요.\n",
    "\n",
    "기계가 보기에는 스펠링이 같더라도 대문자로 된 단어와 소문자로 된 단어는 서로 다른 단어입니다. 예를 들어 'Good'과 'good'은 기계가 보기에는 다른 단어입니다. 그래서 모든 문장에 대해서 전부 영어로 바꿔주는 작업을 하겠습니다.\n",
    "\n",
    "변환 전 : he is a Good boy !\n",
    "\n",
    "변환 후 : he is a good boy !\n",
    "\n",
    "#### 3. 띄어쓰기 단위로 토큰화를 수행하세요.\n",
    "\n",
    "띄어쓰기 단위로 토큰화를 수행해서 단어를 분리하는 작업을 해주세요. 기계는 이렇게 분리된 토큰들을 각각 하나의 단어로 인식할 수 있게 됩니다.\n",
    "\n",
    "토큰화 전 : 'he is a good boy !'\n",
    "\n",
    "토큰화 후 : ['he', 'is', 'a', 'good', 'boy', '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47484c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a74fe57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 197463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56046</th>\n",
       "      <td>He's strange sometimes.</td>\n",
       "      <td>Il est parfois bizarre.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168770</th>\n",
       "      <td>I think it's time for me to call it quits.</td>\n",
       "      <td>Je pense qu'il est temps pour moi d'en rester là.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10762</th>\n",
       "      <td>I sure hope so.</td>\n",
       "      <td>Je l'espère vraiment.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11598</th>\n",
       "      <td>Relax a moment.</td>\n",
       "      <td>Détendez-vous un instant !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76247</th>\n",
       "      <td>Where's your family, Tom?</td>\n",
       "      <td>Où est ta famille, Tom ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               eng  \\\n",
       "56046                      He's strange sometimes.   \n",
       "168770  I think it's time for me to call it quits.   \n",
       "10762                              I sure hope so.   \n",
       "11598                              Relax a moment.   \n",
       "76247                    Where's your family, Tom?   \n",
       "\n",
       "                                                      fra  \\\n",
       "56046                             Il est parfois bizarre.   \n",
       "168770  Je pense qu'il est temps pour moi d'en rester là.   \n",
       "10762                               Je l'espère vraiment.   \n",
       "11598                          Détendez-vous un instant !   \n",
       "76247                            Où est ta famille, Tom ?   \n",
       "\n",
       "                                                       cc  \n",
       "56046   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "168770  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "10762   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "11598   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "76247   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,sys,copy,time\n",
    "import re\n",
    "\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3d14f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15826</th>\n",
       "      <td>The sea is blue.</td>\n",
       "      <td>La mer est bleue.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20362</th>\n",
       "      <td>Somebody saw you.</td>\n",
       "      <td>Quelqu'un vous a vus.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4906</th>\n",
       "      <td>I feel faint.</td>\n",
       "      <td>Je me sens mal.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     eng                    fra\n",
       "15826   The sea is blue.      La mer est bleue.\n",
       "20362  Somebody saw you.  Quelqu'un vous a vus.\n",
       "4906       I feel faint.        Je me sens mal."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 'cc'삭제 및 샘플데이터수 33000개로 한정 \n",
    "lines = lines[['eng', 'fra']][:33000] # 3.3만개 샘플 사용\n",
    "lines.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d16e40",
   "metadata": {},
   "source": [
    "#### 중복값 및 결측치처리 여부\n",
    "\n",
    "-중복값은 나중에 단어빈도파악시 사용되므로, 제거하면 않되나,id 중복은 동일문장이 중복된것이므로. id중복은 파악및 제거필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74c47815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines.index.nunique() 33000\n",
      "id 중복값없습니다.\n"
     ]
    }
   ],
   "source": [
    "## id 중복치 파악및 제거 \n",
    "print(\"lines.index.nunique()\",lines.index.nunique())\n",
    "if lines.index.nunique() < len(lines.index):\n",
    "    pd.Index.drop_duplicates(lines,keep='first', inplace=True) \n",
    "    print(\"lines.index.nunique()\",lines.index.nunique())    \n",
    "    print(\"id 중복제거후 len(lines)\",len(lines)) \n",
    "else:\n",
    "    print(\"id 중복값없습니다.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa1a0b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결측치 개수 0\n",
      "결측치없습니다.\n",
      "결측치 개수 0\n",
      "결측치없습니다.\n"
     ]
    }
   ],
   "source": [
    "## 결측치제거 : 1개라도 있으면, 해당행 전체 제거\n",
    "\n",
    "# encoder_input: eng\n",
    "print(\"결측치 개수\", lines['eng'].isnull().sum())\n",
    "if lines['eng'].isnull().any().any():\n",
    "    lines['eng'].dropna(how='any',inplace=True)\n",
    "    print(\"결측치제거후 결측치 여부\",lines['eng'].isnull().any().any())  \n",
    "else:\n",
    "    print(\"결측치없습니다.\")\n",
    "    \n",
    "# decoder input, label:  fra \n",
    "print(\"결측치 개수\",lines['fra'].isnull().sum())\n",
    "if lines['fra'].isnull().any().any():\n",
    "    lines['fra'].dropna(how='any',inplace=True)\n",
    "    print(\"결측치제거후 결측치 여부\",lines['fra'].isnull().any().any())\n",
    "else:\n",
    "    print(\"결측치없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af1d90",
   "metadata": {},
   "source": [
    "#### 1. 구두점(Punctuation)을 단어와 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "937a5950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Go . \n",
      "1     Go . \n",
      "2     Go . \n",
      "3     Go . \n",
      "4     Hi . \n",
      "5     Hi . \n",
      "6    Run ! \n",
      "7    Run ! \n",
      "8    Run ! \n",
      "9    Run ! \n",
      "Name: eng, dtype: object 32990    Watch your fingers . \n",
      "32991    Watch your fingers . \n",
      "32992    Water is important . \n",
      "32993    We adopted a child . \n",
      "32994    We all cried a lot . \n",
      "32995    We all cried a lot . \n",
      "32996    We all felt hungry . \n",
      "32997    We also found this . \n",
      "32998    We are busy people . \n",
      "32999    We are watching TV . \n",
      "Name: eng, dtype: object\n",
      "0                              Va  ! \n",
      "1                           Marche . \n",
      "2                        En route  ! \n",
      "3                           Bouge  ! \n",
      "4                           Salut  ! \n",
      "5                            Salut . \n",
      "6                           Cours  ! \n",
      "7                          Courez  ! \n",
      "8    Prenez vos jambes à vos cous  ! \n",
      "9                            File  ! \n",
      "Name: fra, dtype: object 32990                   Gare à vos doigts  ! \n",
      "32991                   Gare à tes doigts  ! \n",
      "32992                 L'eau est importante . \n",
      "32993          Nous avons adopté un enfant . \n",
      "32994      Nous avons tous beaucoup pleuré . \n",
      "32995    Nous avons toutes beaucoup pleuré . \n",
      "32996                Nous avions tous faim . \n",
      "32997         Nous avons aussi trouvé ceci . \n",
      "32998         Nous sommes des gens occupés . \n",
      "32999         Nous regardons la télévision . \n",
      "Name: fra, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# encoder input: eng\n",
    "lines['eng'] = lines['eng'].apply(lambda x: re.sub(r\"([!?.])\", r\" \\1 \", x))\n",
    "print(lines['eng'][:10],lines['eng'][-10:]) \n",
    "\n",
    "# decoder input, label:  fra\n",
    "lines['fra'] = lines['fra'].apply(lambda x: re.sub(r\"([!?.])\", r\" \\1 \", x))\n",
    "print(lines['fra'][:10],lines['fra'][-10:]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679dc2e6",
   "metadata": {},
   "source": [
    "#### 2. 소문자로 바꿔주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0ee7ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     go . \n",
      "1     go . \n",
      "2     go . \n",
      "3     go . \n",
      "4     hi . \n",
      "5     hi . \n",
      "6    run ! \n",
      "7    run ! \n",
      "8    run ! \n",
      "9    run ! \n",
      "Name: eng, dtype: object 32990    watch your fingers . \n",
      "32991    watch your fingers . \n",
      "32992    water is important . \n",
      "32993    we adopted a child . \n",
      "32994    we all cried a lot . \n",
      "32995    we all cried a lot . \n",
      "32996    we all felt hungry . \n",
      "32997    we also found this . \n",
      "32998    we are busy people . \n",
      "32999    we are watching tv . \n",
      "Name: eng, dtype: object\n",
      "0                              va  ! \n",
      "1                           marche . \n",
      "2                        en route  ! \n",
      "3                           bouge  ! \n",
      "4                           salut  ! \n",
      "5                            salut . \n",
      "6                           cours  ! \n",
      "7                          courez  ! \n",
      "8    prenez vos jambes à vos cous  ! \n",
      "9                            file  ! \n",
      "Name: fra, dtype: object 32990                   gare à vos doigts  ! \n",
      "32991                   gare à tes doigts  ! \n",
      "32992                 l'eau est importante . \n",
      "32993          nous avons adopté un enfant . \n",
      "32994      nous avons tous beaucoup pleuré . \n",
      "32995    nous avons toutes beaucoup pleuré . \n",
      "32996                nous avions tous faim . \n",
      "32997         nous avons aussi trouvé ceci . \n",
      "32998         nous sommes des gens occupés . \n",
      "32999         nous regardons la télévision . \n",
      "Name: fra, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# encoder input: eng\n",
    "lines['eng'] = lines['eng'].apply(lambda x: str(x).lower())\n",
    "print(lines['eng'][:10],lines['eng'][-10:]) \n",
    "\n",
    "# decoder input, label:  fra\n",
    "lines['fra'] = lines['fra'].apply(lambda x: str(x).lower())\n",
    "print(lines['fra'][:10],lines['fra'][-10:]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce863a43",
   "metadata": {},
   "source": [
    "#### 3. 띄어쓰기 단위로 토큰화수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78153aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6569/3190069281.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  lines['eng'] = lines['eng'].str.replace(\"\\.{2,30}\",\" \")\n",
      "/tmp/ipykernel_6569/3190069281.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  lines['eng'] = lines['eng'].str.replace(\"[' ']+\",\" \")\n",
      "/tmp/ipykernel_6569/3190069281.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  lines['fra'] = lines['fra'].str.replace(\"\\.{2,30}\",\" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [go, ., ]\n",
      "1     [go, ., ]\n",
      "2     [go, ., ]\n",
      "3     [go, ., ]\n",
      "4     [hi, ., ]\n",
      "5     [hi, ., ]\n",
      "6    [run, !, ]\n",
      "7    [run, !, ]\n",
      "8    [run, !, ]\n",
      "9    [run, !, ]\n",
      "Name: eng, dtype: object 32990      [watch, your, fingers, ., ]\n",
      "32991      [watch, your, fingers, ., ]\n",
      "32992      [water, is, important, ., ]\n",
      "32993     [we, adopted, a, child, ., ]\n",
      "32994    [we, all, cried, a, lot, ., ]\n",
      "32995    [we, all, cried, a, lot, ., ]\n",
      "32996     [we, all, felt, hungry, ., ]\n",
      "32997     [we, also, found, this, ., ]\n",
      "32998     [we, are, busy, people, ., ]\n",
      "32999     [we, are, watching, tv, ., ]\n",
      "Name: eng, dtype: object\n",
      "0                                   [va, !, ]\n",
      "1                               [marche, ., ]\n",
      "2                            [en, route, !, ]\n",
      "3                                [bouge, !, ]\n",
      "4                                [salut, !, ]\n",
      "5                                [salut, ., ]\n",
      "6                               [cours , !, ]\n",
      "7                              [courez , !, ]\n",
      "8    [prenez, vos, jambes, à, vos, cous, !, ]\n",
      "9                                 [file, !, ]\n",
      "Name: fra, dtype: object 32990                     [gare, à, vos, doigts, !, ]\n",
      "32991                     [gare, à, tes, doigts, !, ]\n",
      "32992                  [l, eau, est, importante, ., ]\n",
      "32993          [nous, avons, adopté, un, enfant, ., ]\n",
      "32994      [nous, avons, tous, beaucoup, pleuré, ., ]\n",
      "32995    [nous, avons, toutes, beaucoup, pleuré, ., ]\n",
      "32996                 [nous, avions, tous, faim, ., ]\n",
      "32997         [nous, avons, aussi, trouvé, ceci, ., ]\n",
      "32998         [nous, sommes, des, gens, occupés, ., ]\n",
      "32999          [nous, regardons, la, télévision, ., ]\n",
      "Name: fra, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6569/3190069281.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  lines['fra'] = lines['fra'].str.replace(\"[' ']+\",\" \")\n"
     ]
    }
   ],
   "source": [
    "# encoder input: eng\n",
    "lines['eng'] = lines['eng'].str.replace(\"\\.{2,30}\",\" \") \n",
    "lines['eng'] = lines['eng'].str.replace(\",\",\" \") \n",
    "lines['eng'] = lines['eng'].str.replace(\"[' ']+\",\" \")\n",
    "lines['eng'] = lines['eng'].str.split(' ')\n",
    "print(lines['eng'][:10],lines['eng'][-10:])\n",
    "\n",
    "# decoder input, label:  fra\n",
    "lines['fra'] = lines['fra'].str.replace(\"\\.{2,30}\",\" \") \n",
    "lines['fra'] = lines['fra'].str.replace(\",\",\" \") \n",
    "lines['fra'] = lines['fra'].str.replace(\"[' ']+\",\" \")\n",
    "lines['fra'] = lines['fra'].str.split(' ')\n",
    "print(lines['fra'][:10],lines['fra'][-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "667e2d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 샘플데이터 변수명 지정\n",
    "encoder_input = lines['eng']\n",
    "decoder_input = lines['fra']\n",
    "decoder_label = lines['fra']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6163571c",
   "metadata": {},
   "source": [
    "### Step 2. 디코더의 문장에 시작 토큰과 종료 토큰을 넣어주세요.\n",
    "\n",
    "글자 단위 번역기를 구현할 때와 마찬가지로 디코더의 입력 시퀀스 맨 앞에는 시작을 의미하는 토큰인 <sos>가 필요합니다. 그리고 교사 강요를 수행할 때, 디코더의 실제값이 되는 디코더의 레이블 시퀀스에는 종료를 의미하는 종료 토큰 <eos>가 필요합니다.\n",
    "예를 들어 번역 문장이 \"Courez!\" 였다고 한다면, Step 1을 거친 후에는 다음과 같은 결과를 얻습니다.\n",
    "\n",
    "Step 1을 수행한 후 : ['courez', '!']\n",
    "    \n",
    "이 문장에 대해서 각각 디코더의 입력 시퀀스와 레이블 시퀀스를 만들면 다음과 같습니다.\n",
    "\n",
    "입력 시퀀스 : ['<sos>', 'courez', '!']\n",
    "    \n",
    "레이블 시퀀스 : ['courez', '!', '<eos>']\n",
    "    \n",
    "참고로 Step 2가 반드시 Step 1이 끝난 후에 이루어질 필요는 없습니다!\n",
    "Step 1을 수행하는 중간에 수행해도 상관없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e43d75a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "lines          eng                             fra\n",
      "0  [go, ., ]         [<SOS>, va, !, , <EOS>]\n",
      "1  [go, ., ]     [<SOS>, marche, ., , <EOS>]\n",
      "2  [go, ., ]  [<SOS>, en, route, !, , <EOS>]\n",
      "3  [go, ., ]      [<SOS>, bouge, !, , <EOS>]\n",
      "4  [hi, ., ]      [<SOS>, salut, !, , <EOS>]\n",
      "decoder_input1 [['<SOS>', 'va', '!', ''], ['<SOS>', 'marche', '.', ''], ['<SOS>', 'en', 'route', '!', ''], ['<SOS>', 'bouge', '!', ''], ['<SOS>', 'salut', '!', '']] [['va', '!', '', '<EOS>'], ['marche', '.', '', '<EOS>'], ['en', 'route', '!', '', '<EOS>'], ['bouge', '!', '', '<EOS>'], ['salut', '!', '', '<EOS>']]\n"
     ]
    }
   ],
   "source": [
    "# 디코더 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '<SOS>'\n",
    "eos_token = '<EOS>'\n",
    "lines.fra = lines.fra.apply(lambda x : ['<SOS>'] + x + ['<EOS>'])\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "print(\"lines\", lines[:5])\n",
    "\n",
    "# decoder input, label\n",
    "decoder_input1 = [[word for word in line if word != '<EOS>'] for line in decoder_input]\n",
    "decoder_label1 = [[word for word in line if word != '<SOS>'] for line in decoder_label]\n",
    "print(\"decoder_input1\", decoder_input1[:5],decoder_label1[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c42af6",
   "metadata": {},
   "source": [
    "### Step 3. 케라스의 토크나이저로 텍스트를 숫자로 바꿔보세요.\n",
    "\n",
    "딥러닝 모델은 텍스트가 아닌 숫자를 처리합니다. 케라스 토크나이저를 사용해서 각 단어를 고유한 정수로 바꿔보세요.\n",
    "케라스 토크나이저의 사용법은 아래의 링크에서 2. 케라스(Keras)의 텍스트 전처리에 설명되어 있습니다.\n",
    "\n",
    "위키독스: https://wikidocs.net/31766\n",
    "\n",
    "위 링크의 가이드를 통해서 영어와 프랑스어에 대한 토크나이저를 각각 생성하고, tokenizer.texts_to_sequences()를 사용하여 모든 샘플에 대해서 정수 시퀀스로 변환해보세요.\n",
    "\n",
    "##### 참고: tokenizer인자들\n",
    "tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token=None,\n",
    "    analyzer=None,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83d501d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2866e70",
   "metadata": {},
   "source": [
    "#### 영어 Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e895d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV처리후전체단어집합크기: 4713\n",
      "word2index_eng[:10] \n",
      " 4713 ['', '.', 'i', 'you', '?', 'tom', 'it', 'is', 's', 'a']\n",
      "index2word_eng[:10] \n",
      " 4713 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "word_counts_eng[:10] \n",
      " 4713 ['go', '.', '', 'hi', 'run', '!', 'who', '?', 'wow', 'duck']\n",
      "영어 정수인코딩: 33000 [[29, 2, 1], [29, 2, 1], [29, 2, 1], [29, 2, 1], [762, 2, 1], [762, 2, 1], [197, 24, 1], [197, 24, 1], [197, 24, 1], [197, 24, 1]]\n"
     ]
    }
   ],
   "source": [
    "## 영어 \n",
    "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
    "# Tokening\n",
    "tokenizer =  Tokenizer() #oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(lines['eng']) \n",
    "word2index_eng = tokenizer.word_index\n",
    "index2word_eng = {x[1]: x[0] for x in word2index_eng.items()}\n",
    "print(\"OOV처리후전체단어집합크기:\", len(word2index_eng)) \n",
    "#Tokenizer는 pad용 0번인덱스는 놔두고 넘버링됨, oov은 인덱스1로추가됨,나머지인덱스는 빈도순\n",
    "\n",
    "print(\"word2index_eng[:10] \\n\", len(word2index_eng), list(word2index_eng)[:10])\n",
    "print(\"index2word_eng[:10] \\n\", len(index2word_eng), list(index2word_eng)[:10])\n",
    "print(\"word_counts_eng[:10] \\n\", len(tokenizer.word_counts), list(tokenizer.word_counts)[:10])\n",
    "\n",
    "# 영어 정수인코딩\n",
    "input_text = tokenizer.texts_to_sequences(lines['eng'])\n",
    "print(\"영어 정수인코딩:\", len(input_text), input_text[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e56e85",
   "metadata": {},
   "source": [
    "#### 프랑스어 Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d13916cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV처리후전체단어집합크기: 10308\n",
      "word2index_fra[:10] \n",
      " 10308 ['<sos>', '<eos>', '', '.', 'je', 'est', '?', 'tom', '!', 'pas']\n",
      "index2word_fra[:10] \n",
      " 10308 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "word_counts_fra[:10] \n",
      " 10308 ['<sos>', 'va', '!', '', '<eos>', 'marche', '.', 'en', 'route', 'bouge']\n",
      "프랑스어 정수인코딩: [[1, 74, 9, 3, 2], [1, 343, 4, 3, 2], [1, 25, 479, 9, 3, 2], [1, 703, 9, 3, 2], [1, 744, 9, 3, 2], [1, 744, 4, 3, 2], [1, 3642, 9, 3, 2], [1, 3643, 9, 3, 2], [1, 178, 241, 968, 26, 241, 2790, 9, 3, 2], [1, 1440, 9, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "## 프랑스어: \n",
    "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
    "tokenizer =  Tokenizer() #oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(lines['fra']) \n",
    "word2index_fra = tokenizer.word_index\n",
    "index2word_fra = {x[1]: x[0] for x in word2index_fra.items()}\n",
    "print(\"OOV처리후전체단어집합크기:\", len(word2index_fra)) #pad용1개자동처리?,oov1개더해짐\n",
    "#Tokenizer는 pad용 0번인덱스는 놔두고 넘버링됨, oov은 인덱스1로추가됨,나머지인덱스는 빈도순\n",
    "\n",
    "print(\"word2index_fra[:10] \\n\", len(word2index_fra), list(word2index_fra)[:10])\n",
    "print(\"index2word_fra[:10] \\n\", len(index2word_fra), list(index2word_fra)[:10])\n",
    "print(\"word_counts_fra[:10] \\n\", len(tokenizer.word_counts), list(tokenizer.word_counts)[:10])\n",
    "\n",
    "# 프랑스어 정수인코딩\n",
    "target_text = tokenizer.texts_to_sequences(lines['fra'])\n",
    "print(\"프랑스어 정수인코딩:\",target_text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6404f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 4714\n",
      "프랑스어 단어장의 크기 : 10309\n"
     ]
    }
   ],
   "source": [
    "## 단어장의 크기 \n",
    "eng_vocab_size = len(word2index_eng) +1\n",
    "fra_vocab_size = len(word2index_fra) +1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f91b0df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 9\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "## max_length\n",
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a829b56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 4714\n",
      "프랑스어 단어장의 크기 : 10309\n",
      "영어 시퀀스의 최대 길이 9\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "## 필요 통계치 현황\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad9af3",
   "metadata": {},
   "source": [
    "### Step 4. 임베딩 층(Embedding layer) 사용하기\n",
    "\n",
    "이번에는 입력이 되는 각 단어를 임베딩 층을 사용하여 벡터화하겠습니다.\n",
    "임베딩 층을 사용하는 방법과 그 설명에 대해서는 아래의 링크의 1. 케라스 임베딩 층(Keras Embedding layer) 을 참고하세요.\n",
    "\n",
    "위키독스:https://wikidocs.net/33793\n",
    "\n",
    "실제 번역기 구현을 위해서 사용할 수 있는 인코더 코드의 예시는 다음과 같습니다. 이를 통해서 인코더와 디코더의 임베딩 층을 각각 구현해보세요.\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, Masking\n",
    "\n",
    "인코더에서 사용할 임베딩 층 사용 예시\n",
    "\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "\n",
    "enc_emb =  Embedding(단어장의 크기, 임베딩 벡터의 차원)(encoder_inputs)\n",
    "\n",
    "encoder_lstm = LSTM(hidden state의 크기, return_state=True)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "주의할 점은 인코더와 디코더의 임베딩 층은 서로 다른 임베딩 층을 사용해야 하지만, 디코더의 훈련 과정과 테스트 과정(예측 과정)에서의 임베딩 층은 동일해야 한다는 것입니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d1bbb1",
   "metadata": {},
   "source": [
    "#### Input, target data  분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21e3705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input 33000 [[29, 2, 1], [29, 2, 1], [29, 2, 1], [29, 2, 1], [762, 2, 1]]\n",
      "decoder_input 33000 [[1, 74, 9, 3], [1, 343, 4, 3], [1, 25, 479, 9, 3], [1, 703, 9, 3], [1, 744, 9, 3]]\n",
      "decoder_target 33000 [[74, 9, 3, 2], [343, 4, 3, 2], [25, 479, 9, 3, 2], [703, 9, 3, 2], [744, 9, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "## encoder input\n",
    "encoder_input = input_text\n",
    "\n",
    "## decoder input, decoder input 분리\n",
    "# EOS(종료토큰) 제거하고 decoder_input 만들기\n",
    "decoder_input = [[word for word in line if word != word2index_fra['<eos>']] for line in target_text] \n",
    "# SOS(시작토큰) 제거하고 decoder_target 만들기\n",
    "decoder_target = [[word for word in line if word != word2index_fra['<sos>'] ] for line in target_text]\n",
    "\n",
    "print(\"encoder_input\",len(encoder_input ), encoder_input[:5])\n",
    "print(\"decoder_input\",len(decoder_input ), decoder_input[:5])\n",
    "print(\"decoder_target\",len(decoder_target ), decoder_target[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774bc535",
   "metadata": {},
   "source": [
    "#### padding 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d6831",
   "metadata": {},
   "source": [
    "padding 인자\n",
    "\n",
    "tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='pre',\n",
    "    truncating='pre',\n",
    "    value=0.0\n",
    ") # value= 0.0은,tf.keras의 tokenizer에서,내부 코드로 미리 0번 인덱스를 마련해놓아서, default인 0.0 으로 그냥 놔두어도 됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cb2c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Masking,BatchNormalization,Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd54bb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 9)\n",
      "영어패딩 결과3개 : [[29  2  1  0  0  0  0  0  0]\n",
      " [29  2  1  0  0  0  0  0  0]\n",
      " [29  2  1  0  0  0  0  0  0]]\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 17)\n",
      "프랑스어 입력패딩 결과3개 : [[  1  74   9   3   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1 343   4   3   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1  25 479   9   3   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 17)\n",
      "프랑스어 출력패딩결과3개 : [[ 74   9   3   2   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [343   4   3   2   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 25 479   9   3   2   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "## padding 처리\n",
    "\n",
    "# encoder : 영어\n",
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "\n",
    "# decoder : 프랑스어\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('영어패딩 결과3개 :',encoder_input[:3] )\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 입력패딩 결과3개 :',decoder_input[:3] )\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))\n",
    "print('프랑스어 출력패딩결과3개 :',decoder_target[:3] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5312177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 Train input data 크기(shape) : (30000, 9)\n",
      "프랑스어 Train input data 크기(shape) : (30000, 17)\n",
      "프랑스어 Train target data 크기(shape) : (30000, 17)\n",
      "영어 Test input data 크기(shape) : (3000, 9)\n",
      "프랑스어 Test input data 크기(shape) : (3000, 17)\n",
      "프랑스어 Test target data 크기(shape) : (3000, 17)\n"
     ]
    }
   ],
   "source": [
    "## train, test data 분리\n",
    "# test data 개수\n",
    "n_of_test = 3000\n",
    "\n",
    "# permutation\n",
    "indices_permutation = np.random.permutation(len(encoder_input))\n",
    "#indices_permutation = np.arange(len(encoder_input))\n",
    "shuffled_encoder_input = encoder_input[indices_permutation]\n",
    "shuffled_decoder_input = decoder_input[indices_permutation]\n",
    "shuffled_decoder_target = decoder_target[indices_permutation]\n",
    "\n",
    "# train, test data\n",
    "encoder_input_train = shuffled_encoder_input[:-n_of_test]\n",
    "decoder_input_train = shuffled_decoder_input[:-n_of_test]\n",
    "decoder_target_train = shuffled_decoder_target[:-n_of_test]\n",
    "\n",
    "encoder_input_test = shuffled_encoder_input[-n_of_test:]\n",
    "decoder_input_test = shuffled_decoder_input[-n_of_test:]\n",
    "decoder_target_test = shuffled_decoder_target[-n_of_test:]\n",
    "\n",
    "print('영어 Train input data 크기(shape) :',np.shape(encoder_input_train))\n",
    "print('프랑스어 Train input data 크기(shape) :',np.shape(decoder_input_train))\n",
    "print('프랑스어 Train target data 크기(shape) :',np.shape(decoder_target_train))\n",
    "\n",
    "print('영어 Test input data 크기(shape) :',np.shape(encoder_input_test))\n",
    "print('프랑스어 Test input data 크기(shape) :',np.shape(decoder_input_test))\n",
    "print('프랑스어 Test target data 크기(shape) :',np.shape(decoder_target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ceec77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## embedding layer설정\n",
    "\n",
    "#1. encoder embedding layer: 영어 embedding layer\n",
    "enbedding_dim = 256\n",
    "enc_emb = Embedding(eng_vocab_size, enbedding_dim)\n",
    "\n",
    "#2. decoder embedding layer: 프랑스어 embedding layer\n",
    "enbedding_dim = 256\n",
    "dec_emb = Embedding(fra_vocab_size, enbedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da98d37",
   "metadata": {},
   "source": [
    "### Step 5. 모델 구현하기\n",
    "단어 단위 번역기의 모델을 완성시켜보세요! 이때는 label이 integer 값이므로 categorical entropy loss가 아닌 sparse categorical entropy loss를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76e6e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe57320",
   "metadata": {},
   "source": [
    "#### 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a37d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper params\n",
    "eng_vocab_size = eng_vocab_size\n",
    "fra_vocab_size = fra_vocab_size\n",
    "enbedding_dim = 256\n",
    "hidden_size1 = 256\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "batch_size= 256\n",
    "epochs= 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc11915b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    1206784     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 256)    2639104     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, 256)    1024        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 256)    1024        embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 256)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 256)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 525312      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      dropout_1[0][0]                  \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 10309)  2649413     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 7,547,973\n",
      "Trainable params: 7,546,949\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(eng_vocab_size, enbedding_dim)(encoder_inputs)\n",
    "#enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "enc_bn = BatchNormalization()(enc_emb)\n",
    "enc_do = Dropout(dropout_rate)(enc_bn)\n",
    "encoder_lstm = LSTM(hidden_size1, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_do) \n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb = Embedding(fra_vocab_size, enbedding_dim)(decoder_inputs)\n",
    "#dec_masking = Masking(mask_value=0.0)(dec_emb) # 패딩 0은 연산에서 제외\n",
    "dec_bn = BatchNormalization()(dec_emb)\n",
    "dec_do = Dropout(dropout_rate)(dec_bn)\n",
    "decoder_lstm = LSTM(hidden_size1, return_sequences = True, return_state=True)\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(dec_do ,initial_state = encoder_states)\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "# model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "rmsprop = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "model.compile(optimizer=rmsprop, loss=\"sparse_categorical_crossentropy\", metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa660c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "118/118 [==============================] - 12s 61ms/step - loss: 1.8985 - acc: 0.7538 - val_loss: 3.3013 - val_acc: 0.6024\n",
      "Epoch 2/25\n",
      "118/118 [==============================] - 7s 56ms/step - loss: 1.1028 - acc: 0.8286 - val_loss: 2.0802 - val_acc: 0.6743\n",
      "Epoch 3/25\n",
      "118/118 [==============================] - 7s 56ms/step - loss: 0.9111 - acc: 0.8503 - val_loss: 1.6797 - val_acc: 0.6905\n",
      "Epoch 4/25\n",
      "118/118 [==============================] - 7s 56ms/step - loss: 0.7956 - acc: 0.8642 - val_loss: 1.0420 - val_acc: 0.8035\n",
      "Epoch 5/25\n",
      "118/118 [==============================] - 7s 56ms/step - loss: 0.7097 - acc: 0.8748 - val_loss: 0.8015 - val_acc: 0.8684\n",
      "Epoch 6/25\n",
      "118/118 [==============================] - 7s 56ms/step - loss: 0.6401 - acc: 0.8840 - val_loss: 0.7333 - val_acc: 0.8766\n",
      "Epoch 7/25\n",
      "118/118 [==============================] - 7s 57ms/step - loss: 0.5831 - acc: 0.8918 - val_loss: 0.6965 - val_acc: 0.8833\n",
      "Epoch 8/25\n",
      "118/118 [==============================] - 7s 57ms/step - loss: 0.5358 - acc: 0.8992 - val_loss: 0.6722 - val_acc: 0.8849\n",
      "Epoch 9/25\n",
      "118/118 [==============================] - 7s 57ms/step - loss: 0.4942 - acc: 0.9058 - val_loss: 0.6471 - val_acc: 0.8909\n",
      "Epoch 10/25\n",
      "118/118 [==============================] - 7s 57ms/step - loss: 0.4576 - acc: 0.9111 - val_loss: 0.6284 - val_acc: 0.8933\n",
      "Epoch 11/25\n",
      "118/118 [==============================] - 7s 57ms/step - loss: 0.4262 - acc: 0.9166 - val_loss: 0.6150 - val_acc: 0.8938\n",
      "Epoch 12/25\n",
      "118/118 [==============================] - 7s 57ms/step - loss: 0.3990 - acc: 0.9207 - val_loss: 0.6035 - val_acc: 0.8962\n",
      "Epoch 13/25\n",
      "118/118 [==============================] - 7s 57ms/step - loss: 0.3758 - acc: 0.9246 - val_loss: 0.5992 - val_acc: 0.8972\n",
      "Epoch 14/25\n",
      "118/118 [==============================] - 7s 58ms/step - loss: 0.3557 - acc: 0.9279 - val_loss: 0.5882 - val_acc: 0.8994\n",
      "Epoch 15/25\n",
      "118/118 [==============================] - 7s 57ms/step - loss: 0.3373 - acc: 0.9306 - val_loss: 0.5821 - val_acc: 0.8995\n",
      "Epoch 16/25\n",
      "118/118 [==============================] - 7s 58ms/step - loss: 0.3208 - acc: 0.9335 - val_loss: 0.5791 - val_acc: 0.9004\n",
      "Epoch 17/25\n",
      "118/118 [==============================] - 7s 58ms/step - loss: 0.3055 - acc: 0.9362 - val_loss: 0.5765 - val_acc: 0.9009\n",
      "Epoch 18/25\n",
      "118/118 [==============================] - 7s 58ms/step - loss: 0.2924 - acc: 0.9381 - val_loss: 0.5705 - val_acc: 0.9002\n",
      "Epoch 19/25\n",
      "118/118 [==============================] - 7s 58ms/step - loss: 0.2806 - acc: 0.9401 - val_loss: 0.5681 - val_acc: 0.9015\n",
      "Epoch 20/25\n",
      "118/118 [==============================] - 7s 58ms/step - loss: 0.2681 - acc: 0.9421 - val_loss: 0.5647 - val_acc: 0.9022\n",
      "Epoch 21/25\n",
      "118/118 [==============================] - 7s 58ms/step - loss: 0.2579 - acc: 0.9436 - val_loss: 0.5628 - val_acc: 0.9029\n",
      "Epoch 22/25\n",
      "118/118 [==============================] - 7s 58ms/step - loss: 0.2488 - acc: 0.9450 - val_loss: 0.5647 - val_acc: 0.9025\n",
      "Epoch 23/25\n",
      "118/118 [==============================] - 7s 58ms/step - loss: 0.2403 - acc: 0.9467 - val_loss: 0.5658 - val_acc: 0.9027\n",
      "Epoch 24/25\n",
      "118/118 [==============================] - 7s 58ms/step - loss: 0.2337 - acc: 0.9476 - val_loss: 0.5627 - val_acc: 0.9028\n",
      "Epoch 25/25\n",
      "118/118 [==============================] - 7s 59ms/step - loss: 0.2263 - acc: 0.9487 - val_loss: 0.5665 - val_acc: 0.9028\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=batch_size, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a5db4",
   "metadata": {},
   "source": [
    "#### 학습결과: 학습도 잘되고, Overfitting도 별로 없는 양호한 결과였슴\n",
    "\n",
    "loss: 0.2263\n",
    "    \n",
    "val_loss: 0.5665\n",
    "    \n",
    "Accuracy: 94.87%\n",
    "    \n",
    "val_Accuracy: 90.28%    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d35540df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Loss\n",
    "history1 = history.history\n",
    "print(history1.keys()) \n",
    "\n",
    "loss = history1['loss']\n",
    "val_loss = history1['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# LOSS\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Seq2seq Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1ea1434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Accuracy\n",
    "history1 = history.history\n",
    "acc = history1['acc']\n",
    "val_acc = history1['val_acc']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Accuracy\n",
    "plt.plot(epochs, acc, 'bo', label='Training_Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation_Accuracy')\n",
    "plt.title('Seq2seq Training and validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf18bbd0",
   "metadata": {},
   "source": [
    "### Step 6. 모델 평가하기\n",
    "\n",
    "단어 단위 번역기를 이용하여 훈련 데이터의 샘플과 테스트 데이터의 샘플로 번역 문장을 만들어보고 정답 문장과 번역 문장을 비교해보세요. 이전 스텝들에서 우리가 공부했던 모델의 경우 글자 단위에서 구현된 번역기이며 현재 프로젝트를 진행할 때 사용하는 모델은 단어 단위에서 구현되는 번역기입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9190ca0",
   "metadata": {},
   "source": [
    "#### (1) 테스트 모드의 인코더 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52e6c67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, None, 256)         1206784   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 525312    \n",
      "=================================================================\n",
      "Total params: 1,733,120\n",
      "Trainable params: 1,732,608\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## 테스트 모드의 인코더 만들기\n",
    "\n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e963cda3",
   "metadata": {},
   "source": [
    "#### (2) 테스트모드의 디코더 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7db0faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 테스트모드의 디코더 설계\n",
    "\n",
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size1,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(hidden_size1,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "\n",
    "# decoder_lstm()안에는 decoder_input 대신에 기 학습된 모델의 \n",
    "#==> 직전 layer인 dropout layer로 연결시켰습니다\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(model.layers[7].output, initial_state = decoder_states_inputs)\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states = [state_h, state_c]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c56b8",
   "metadata": {},
   "source": [
    "#### (3) 테스트모드의 디코더 출력층 재설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45d18207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 256)    2639104     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 256)    1024        embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 256)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      dropout_1[0][0]                  \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 10309)  2649413     lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,814,853\n",
      "Trainable params: 5,814,341\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## 테스트모드의 디코더 출력층 재설계\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "051eb8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 기존 사전 사용활용\n",
    "word2index_eng = word2index_eng\n",
    "index2word_eng = index2word_eng\n",
    "word2index_fra = word2index_fra\n",
    "index2word_fra = index2word_fra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3dc054",
   "metadata": {},
   "source": [
    "#### (4) 테스트 모드의 예측함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6a6bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 테스트 모드의 예측함수 만들기\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1,1)) \n",
    "    target_seq[0, 0] = word2index_fra['<sos>']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index2word_fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # 에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장     \n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc300b86",
   "metadata": {},
   "source": [
    "#### (5) 테스트 모드의 5개단어 출력결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3578b667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: ['go', '.', '']\n",
      "정답 문장: ['bouge', '!', '']\n",
      "번역기가 번역한 문장:  va !  <eos\n",
      "-----------------------------------\n",
      "입력 문장: ['hello', '!', '']\n",
      "정답 문장: ['bonjour', '!', '']\n",
      "번역기가 번역한 문장:  salut !  <eos\n",
      "-----------------------------------\n",
      "입력 문장: ['got', 'it', '?', '']\n",
      "정답 문장: ['t', 'as', 'capté\\u202f', '?', '']\n",
      "번역기가 번역한 문장:  est-ce ?  <eos\n",
      "-----------------------------------\n",
      "입력 문장: ['hang', 'on', '.', '']\n",
      "정답 문장: ['tiens', 'bon', '!', '']\n",
      "번역기가 번역한 문장:  tiens bon !  <eos\n",
      "-----------------------------------\n",
      "입력 문장: ['here', 's', '$5', '.', '']\n",
      "정답 문장: ['voilà', 'cinq', 'dollars', '.', '']\n",
      "번역기가 번역한 문장:  voilà cinq ici .\n"
     ]
    }
   ],
   "source": [
    "## 테스트 모드의 5개단어 출력결과\n",
    "\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3fd446",
   "metadata": {},
   "source": [
    "#### (6) 출력결과 Naver 사전해석과 비교"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad54cc57",
   "metadata": {},
   "source": [
    "입력 문장: ['go', '.', ''],     네이버영어사전 해석: 가다\n",
    "정답 문장: ['bouge', '!', ''],     네이버프랑스어사전 해석: 움직이다, 이동하다\n",
    "번역기가 번역한 문장:  va !  <eos, 네이버프랑스어사전 해석: 가다\n",
    "\n",
    "입력 문장: ['hello', '!', ''],   네이버영어사전 해석:  안녕하세요\n",
    "정답 문장: ['bonjour', '!', ''],   네이버프랑스어사전 해석: 안녕하세요\n",
    "번역기가 번역한 문장:   salut !  <eos , 네이버프랑스어사전 해석: 안녕하세요, 구원             \n",
    "입력 문장: ['got', 'it', '?', ''],           네이버영어사전 해석: 알겠어요?\n",
    "정답 문장: ['t', 'as', 'capté\\u202f', '?', ''], 네이버프랑스어사전 해석: 잡았어?\n",
    "번역기가 번역한 문장:  est-ce que ça a , 네이버프랑스어사전해석:이게무슨의미있는일인가\n",
    "        \n",
    "입력 문장: ['hang', 'on', '.', ''],   네이버영어사전 해석: 기다려\n",
    "정답 문장: ['tiens', 'bon', '!', ''],   네이버프랑스어사전 해석: 가만있어\n",
    "번역기가 번역한 문장:  attendez un peu, 네이버프랑스어사전 해석: 잠깐만\n",
    " \n",
    "입력 문장: ['here', 's', '$5', '.', ''],        네이버영어사전 해석:여기5달러있다 \n",
    "정답 문장: ['voilà', 'cinq', 'dollars', '.', ''], 네이버프랑스어사전해석:여기5달러있다\n",
    "번역기가 번역한 문장:  voilà cinq ici ,     네이버프랑스어사전 해석: 여기5개가 있다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc168ae",
   "metadata": {},
   "source": [
    "#### (7) 번역기 해석결과 요약\n",
    "\n",
    "-5개 단어 모두 정답과딱맞아 떨어지지는 않았으나, \n",
    " 한국어해석으로는 거의 같은 뜻으로 해석되었습니다.\n",
    "\n",
    "-작은 모델임을 감안했을때 결과가 훌륭한 편입니다.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce7e66",
   "metadata": {},
   "source": [
    "## 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363ea010",
   "metadata": {},
   "source": [
    "본 프로젝트는 자연어처리 모델인 seq2seq 모델로서, 단어단위로 구현된 번역기를 만들어보는 프로젝트였습니다.\n",
    "프랑스어와 영어의 병렬 코퍼스인 fra-eng.zip을 다운받아 사용하였는데, \n",
    "33000개 중 3000개는 테스트 데이터로 분리하여 모델을 학습한 후에 번역을 테스트 하는\n",
    "용도로 사용하였습니다. \n",
    "\n",
    "#### Step 1. 정제, 정규화, 전처리 (영어, 프랑스어 모두!)\n",
    "\n",
    "먼저 1단계에서는 33000개의 문장들을 tokenize하기 편하게, 전처리하기위해서,\n",
    "id 상 중복된 문장들을 제거하기 위해, id상 중복된 문장들을 파악했으나, 없었습니다.\n",
    "그리고, 결측치제거하려고, 결측치파악을 하였으나, 역시 없었습니다.\n",
    "그 다음 데이터 정제를 위해서, 구두점 분리, 소문자로 변경작업, 전체 문장을\n",
    "단어(토큰)별 띄어쓰기 리스트로 만들어서, 기계가 처리하기 깔끔하게 처리했습니다. \n",
    "\n",
    "#### Step 2. 디코더의 문장에 시작 토큰과 종료 토큰을 추가\n",
    "\n",
    "상기에서 깔끔하게 단어별 리스트로 정제된 문장들중 디코더에서 처리할 프랑스어 문장에대해,\n",
    "문장맨앞에 <SOS> , 맨뒤에 <EOS> 토큰을 입력하였습니다. \n",
    "\n",
    "#### Step 3. 케라스의 토크나이저로 텍스트를 숫자로 변환.\n",
    "\n",
    "Keras의 Tokenizer를 사용해서,전체 문장의 각 단어들을 고유한 정수로 처리합니다 \n",
    "먼저 영어입니다. 영어 Tokenizer를 생성하고서,tokenizer.fit_on_texts(lines['eng'])로\n",
    "영어를 출현빈도순으로 토큰화한후,4713개의 단어가 등록된 word2index_eng, \n",
    "index2word_eng 사전을 만들었습니다.\n",
    "그리고,tokenizer.texts_to_sequences()를 사용하여 모든 샘플에 대해서 정수 시퀀스로\n",
    "변환했습니다.\n",
    "\n",
    "다음의 프랑스어도 동일한 과정을 거쳐서, 10308개의 단어가 등록된 word2index_fra,\n",
    "index2word_fra 사전을 만들었습니다.\n",
    "그리고,tokenizer.texts_to_sequences()를 사용하여 모든 샘플에 대해서 정수 시퀀스로\n",
    "변환했습니다.\n",
    "\n",
    "Tokenizer에는 나중에 처리할 padding용 0번인덱스는 놔두고, 1부터의 고유한 정수인덱스로 \n",
    "넘버링하는 기능이 있습니다.\n",
    "단어장의 크기는 padding용 0번을 포함하여 계산하였습니다. \n",
    "영어 단어장인 eng_vocab_size 는 len(word2index_eng) +1 하여 4714개,\n",
    "프랑스어 단어장인 fra_vocab_size 는 len(word2index_fra) +1하여 10309개입니다.\n",
    "그리고 padding시에 활용할 한문장의 최대길이(max_len)는 영어가 9개, 프랑스어가 17개입니다.\n",
    "\n",
    "#### Step 4. 임베딩 층(Embedding layer) 사용하기\n",
    "\n",
    "이번에는 전단계에서, 단어별로 토크화한 문장들을 임베딩 층을 사용하여\n",
    "벡터화해보겠습니다.\n",
    "이때 인코더와 디코더의 임베딩 층은 서로 다른 임베딩 층을 사용해야 하지만,\n",
    "디코더의 훈련 과정과 테스트 과정(예측 과정)에서의 임베딩 층은 동일해야 한다는 점을 \n",
    "주의해서, 작업했습니다.\n",
    "\n",
    "이 단계에서, 데이터를 encoder_input,decoder_input,decoder_target을 분리하고서,\n",
    "decoder input에는 시작을 의미하는 <SOS>만 남기고,<EOS>는 제거하였으며,\n",
    "teacher forcing의 수행에 쓰이는 decoder target은 종료를 의미하는 <EOS>만 남기고, \n",
    "<SOS>는 제거하였습니다.\n",
    "\n",
    "디음은 pad_sequences()메서드로 영어, 프랑스어 모두 Padding처리하였습니다. \n",
    "영어는 최대길이 9개단어로 한문장의 길이를 동일하게 Padding하고, 프랑스어는 17개를 최대길이로 \n",
    "Padding 처리하였습니다.\n",
    "\n",
    "이렇게 준비가 완료된 데이터들을 train, test data 로 분리하였습니다.\n",
    "test data 개수는 33000개중 3000개로 하였고, 데이터의 고른 분포를 위해서,\n",
    "분리전에 미리 permutation해서,고르게 분포시켰습니다. \n",
    "\n",
    "그리고서,다음 단계인 모델구현에서 사용될 언어별 Embedding layer을 준비하였습니다.\n",
    "encoder embedding layer: 영어 embedding layer\n",
    "enc_emb = Embedding(eng_vocab_size, enbedding_dim)\n",
    "\n",
    "decoder embedding layer: 프랑스어 embedding layer\n",
    "dec_emb = Embedding(fra_vocab_size, enbedding_dim)\n",
    "\n",
    "\n",
    "#### Step 5. 모델 구현\n",
    "\n",
    "모델 구현에는 인코더와 디코더로 크게 나뉘고, 각각 임베딩층, 배치노말, 드롭아웃,\n",
    "LSTM레이어를 기본으로 배치했습니다.  배치모말레이어는 배치단위로 정규화를해서, 학습이 잘되도록하고, \n",
    "드롭아웃레이어는 과대적합을 피하기 위해서였습니다.\n",
    "\n",
    "하이퍼파람은 \n",
    "eng_vocab_size = 4714\n",
    "fra_vocab_size = 10309\n",
    "enbedding_dim = 256\n",
    "hidden_size1 = 256\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "batch_size= 256\n",
    "epochs= 25\n",
    "로 설정하였습니다.\n",
    "\n",
    "인코더는\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(eng_vocab_size, enbedding_dim)(encoder_inputs)\n",
    "enc_bn = BatchNormalization()(enc_emb)\n",
    "enc_do = Dropout(dropout_rate)(enc_bn)\n",
    "encoder_lstm = LSTM(hidden_size1, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_do) \n",
    "encoder_states = [state_h, state_c]\n",
    "​\n",
    "디코더는\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb = Embedding(fra_vocab_size, enbedding_dim)(decoder_inputs)\n",
    "dec_bn = BatchNormalization()(dec_emb)\n",
    "dec_do = Dropout(dropout_rate)(dec_bn)\n",
    "decoder_lstm = LSTM(hidden_size1, return_sequences = True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(dec_do ,initial_state = encoder_states)\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "로 하여서,\n",
    "인코더와는 다르게,return_sequences = True, return_state=True로 놓아서,\n",
    "decoder_outputs이 다음 time step의 input,hidden state로 들어가게 하였습니다.   \n",
    "\n",
    "옵티마이저는 tensorflow.keras.optimizers.RMSprop()를 직접써서, 학습율을 0.001로 설정\n",
    "하였고, 손실함수는 정수형 벡터라서,sparse_categorical_crossentropy를 사용하였습니다. \n",
    "\n",
    "학습은 처음에 epoch 100회로 하였는데, 25회부터 과대적합이 발생되어서,커널 restart후\n",
    "25회로 재 학습하였습니다.\n",
    "학습결과는 학습도 잘되고, Overfitting도 별로 없는 양호한 결과였습니다.\n",
    "loss: 0.2263, val_loss: 0.5665, Accuracy: 94.87%, val_Accuracy: 90.28%\n",
    "\n",
    "학습과정상의 loss, val_loss의 과정과 Accuracy,val_accuracy를 시각화해서\n",
    "val_loss가 우하향하고, val_accccuracy가 우상향하는 모습을 보았습니다.\n",
    "\n",
    "\n",
    "#### Step 6. 모델 평가하기\n",
    "\n",
    "테스트 모드의 인코더 만들기는 기존의 모델을 그대로 사용하였습니다\n",
    "그런데, 디코더모델은 teacher forcing이 아닌, 직전 timetep의 out을 그대로 입력으로 받도록\n",
    "재설계하였고, decoder_lstm()안에는 decoder_input 대신에 기 학습된 모델의 \n",
    "직전 layer인 dropout layer로 연결시켰습니다\n",
    "\n",
    "그리고, 테스트모드에서의 인코더와 재설계된 디코드를 이용하여,\n",
    "입력단어(영어)를 받아서, 예측단어(프랑스어로 번역단어)를 출력하는\n",
    "decode_sequence()함수를 만들었습니다.\n",
    "\n",
    "이 함수에 5개단어 출력결과를, Naver사전에서도 재확인해보았습니다(입력단어별 맨우측기재)\n",
    "\n",
    "###### 1. 입력 문장: ['go', '.', ''],     네이버영어사전 해석: 가다\n",
    "    \n",
    "정답 문장: ['bouge', '!', ''],     네이버프랑스어사전 해석: 움직이다, 이동하다\n",
    "    \n",
    "번역기가 번역한 문장:  va !  <eos, 네이버프랑스어사전 해석: 가다\n",
    "​\n",
    "###### 2.입력 문장: ['hello', '!', ''],   네이버영어사전 해석:  안녕하세요\n",
    "                          \n",
    "정답 문장: ['bonjour', '!', ''],   네이버프랑스어사전 해석: 안녕하세요\n",
    "                          \n",
    "번역기가 번역한 문장:   salut !  <eos , 네이버프랑스어사전 해석: 안녕하세요, 구원             \n",
    "        \n",
    "###### 3. 입력 문장: ['got', 'it', '?', ''],           네이버영어사전 해석: 알겠어요?\n",
    "    \n",
    "정답 문장: ['t', 'as', 'capté\\u202f', '?', ''], 네이버프랑스어사전 해석: 잡았어?\n",
    "    \n",
    "번역기가 번역한 문장:  est-ce que ça a , 네이버프랑스어사전 해석: 이게무슨의미있는일인가다\n",
    "        \n",
    "###### 4.입력 문장: ['hang', 'on', '.', ''],   네이버영어사전 해석: 기다려\n",
    "    \n",
    "정답 문장: ['tiens', 'bon', '!', ''],   네이버프랑스어사전 해석: 가만있어\n",
    "    \n",
    "번역기가 번역한 문장:  attendez un peu, 네이버프랑스어사전 해석: 잠깐만\n",
    "    \n",
    "    \n",
    "###### 5.입력 문장: ['here', 's', '$5', '.', ''],        네이버영어사전 해석:여기 5달러 있다 \n",
    "\n",
    "정답 문장: ['voilà', 'cinq', 'dollars', '.', ''], 네이버프랑스어사전 해석:여기 5달러 있다\n",
    "\n",
    "번역기가 번역한 문장:  voilà cinq ici ,           네이버프랑스어사전 해석: 여기 5개가 있다.       \n",
    "        \n",
    "​\n",
    "5개단어 번역기 해석결과는 5개 단어 모두 정답과딱맞아 떨어지지는 않았으나, \n",
    "한국어해석으로는 거의 같은 뜻으로 해석되었습니다.\n",
    "작은 모델임을 감안했을때 결과가 훌륭한 편입니다\n",
    "\n",
    "본 프로젝트를 통하여, 자연어처리모델인 seq2seq모델을 처음부터 끝까지 만들어 본 좋은\n",
    "기회였습니다.\n",
    "좀 더 큰 모델로 좀 더 기간을 투입하면, 꽤 훌륭한 모델을 만들수 있을 것 같은 자신감이 \n",
    "생겼습니다. 감사합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a34b15e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9610ecaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25a02e0c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "루브릭\n",
    "\n",
    "아래의 기준을 바탕으로 프로젝트를 평가합니다.\n",
    "\n",
    "평가문항\t상세기준\n",
    "1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다.\n",
    "\n",
    "구두점, 대소문자, 띄어쓰기 등 번역기 모델에 요구되는 전처리가 정상적으로 진행되었다.\n",
    "2. seq2seq 기반의 번역기 모델이 정상적으로 구동된다.\n",
    "\n",
    "seq2seq 모델 훈련결과를 그래프로 출력해보고, validation loss그래프가 우하향하는 경향성을 보이며 학습이 진행됨이 확인되었다.\n",
    "3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.\n",
    "\n",
    "테스트용 디코더 모델이 정상적으로 만들어졌으며, input(영어)와 output(프랑스어) 모두 한글로 번역해서 결과를 출력해보았고, 둘의 내용이 유사함을 확인하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f1361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd41318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
