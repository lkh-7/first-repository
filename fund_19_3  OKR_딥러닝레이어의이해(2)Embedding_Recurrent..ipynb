{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb811e1",
   "metadata": {},
   "source": [
    "### OKR\n",
    "\n",
    "#### 19-2. 임베딩의 종류와 방식을 이해할 수 있다.\n",
    " - 단어를 수치로 표현할 수 있는 방법은 대표적으로 어떤 것이 있는지 찾아보자.\n",
    " - 각 방식이 갖는 장단점에는 어떤 것들이 있을지 이야기해 보자.\n",
    "\" - 만약 내가 사과, 오렌지, 오이, 바나나를 숫자로 구분한다면 어떤식으로 각 단어의 특징을 표현할 수 \n",
    "있을지 생각해보고 팀원들과 나눠보았다.\"\n",
    "\n",
    "\n",
    "#### 19-3. Embedding layer에서 일어나는 연산을 이해할 수 있다.\n",
    " - tf.keras.Dense를 검색해보고, 파라미터의 default로 어떤 것들이 들어가고 아웃풋이 어떻게 나오는지 확인해보자.\n",
    " - embedding에서 연산하는 방식을 생각해보고 이것이 어떻게 동작하는지 설명해보자.\n",
    "\" - embedding에서 input은 어떤 형태인가?\n",
    " - weight는 어떤 형태인가?\"\n",
    "    \n",
    "    \n",
    "#### 19-4. RNN계열의 모델에 대해 이해할 수 있다.\n",
    " - RNN에서 한 레이어 안에 가중치행렬은 총 몇개가 있는지 논의해보자.\n",
    " - 각 가중치를 step별로 따로 구하지 않고 하나의 가중치를 공유한다는 것이 어떤 의미인지 논의해보자.\n",
    " - lstm에서 activation function으로 sigmoid, tanh를 사용하는 것의 의미에 대해 설명해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103642ff",
   "metadata": {},
   "source": [
    "#### 19-2. 임베딩의 종류와 방식을 이해할 수 있다.\n",
    " - 단어를 수치로 표현할 수 있는 방법은 대표적으로 어떤 것이 있는지 찾아보자.\n",
    "    \n",
    "    *희소표현(sparse representation)\n",
    "    *분산표현(distributed representation)   \n",
    "    \n",
    " - 각 방식이 갖는 장단점에는 어떤 것들이 있을지 이야기해 보자.\n",
    "\n",
    "    *희소표현(sparse representation): one-hot encoding 처럼 단순명확하지만, 표시할 단어들이 많을때 엄청난 컴퓨터 자원이 필요함\n",
    "    *분산표현(distributed representation) :  차원을 고정해두고서, 단어들을 벡터로 표현하되, 맥랙에 맞는 단어들을\n",
    "        가까운 거리로 표기하고, 아닌경우 먼거리로 분류하여, 각각의 맥락에따라 분산되어 분포하며,\n",
    "        유사도분류를 통해 특징을 찾을수 있게하는 표현으로 희소표현에 비해 컴퓨터자원효율성이 좋음\n",
    "        embeded layer가 분산표현을 하기위한 것이며,단지 단어만 지칭하지않고, \n",
    "        의미적 유사성까지 컴퓨터가 학습하도록 함\n",
    "        \n",
    "        \n",
    "\" - 만약 내가 사과, 오렌지, 오이, 바나나를 숫자로 구분한다면 어떤식으로 각 단어의 특징을 표현할 수 \n",
    "있을지 생각해보고 팀원들과 나눠보았다.\"\n",
    "\n",
    "   번호를 붙이면 해당단어를 가르키지만, 의미까지 인식할수 있도록하려면, 다른 방법이 필요함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764564f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e24532f",
   "metadata": {},
   "source": [
    "#### 19-3. Embedding layer에서 일어나는 연산을 이해할 수 있다.\n",
    "\n",
    " - tf.keras.Dense를 검색해보고, 파라미터의 default로 어떤 것들이 들어가고 아웃풋이 어떻게 나오는지 확인해보자.\n",
    " \n",
    " tf.keras.layers.Dense(\n",
    "    units,==>  출력공간의 차원수\n",
    "    activation=None,==> 활성화함수,default: 'linear'\n",
    "    use_bias=True,  ==> bias 사용여부, default: True\n",
    "    kernel_initializer='glorot_uniform', ==> 커널가중치행렬의 초기화값 , 'glorot_uniform'\n",
    "    bias_initializer='zeros',==> 바이어스의 초기화값 ,zero\n",
    "    kernel_regularizer=None,, ==> 커널가중치행렬의 규제화값 ,None\n",
    "    bias_regularizer=None,, ==> 바이어스벡터의 규제화값 ,None\n",
    "    activity_regularizer=None,, ==>활성화함수의 규제화값 ,None\n",
    "    kernel_constraint=None,, ==> 커널가중치행렬의 constraint값 ,None\n",
    "    bias_constraint=None,, ==> 바이어스벡터의 constraint값 ,None\n",
    "    **kwargs\n",
    ")\n",
    " \n",
    "  아웃풋: 가중치행렬과 가중치가 선택된 원핫인코딩의 1 이 위치한 곳에서 선택된 가중치배열(linear연산)  \n",
    " \n",
    " - embedding에서 연산하는 방식을 생각해보고 이것이 어떻게 동작하는지 설명해보자.\n",
    " \n",
    "   *embedding layer: 컴퓨터가 읽고 사용할수있는 단어사전 만드는 layer\n",
    "   *임베딩레이어의 weight는 단어의 갯수, 임베딩size(표현의 깊이를 나타내주는 차원수)로 정의됨,\n",
    "   *레이어에 입력으로 들어온 단어를 분산표현으로 연결해주는 역할(weight에서 one-hot-encoding으로 표현된 \n",
    "   특정행을 읽어오는 lookup 테이블역할)\n",
    "      \n",
    "   *동작원리\n",
    "    - embedding layer와 one-hot-encoding가 결합하여 동작함\n",
    "    (1) 단어를 숫자와 key ,value 쌍으로 연결시킨 dict 필요, embedding시킬 문장(sentence)필요,\n",
    "        문장을 dict의 key와 mapping시켜서, 숫자로 구성된 리스트로 만듬\n",
    "    (2) one-hot-encoding\n",
    "    (3) 표현의 깊이 지정: embedding_size ==  distribution_size\n",
    "        distribution_size = 2   # 보기 좋게 2차원으로 분산 표현하도록 하죠!\n",
    "    (4) Dense layer 생성및 실행: 가중치와 one-hot linear 변환    \n",
    "        linear = tf.keras.layers.Dense(units=distribution_size, use_bias=False)\n",
    "        one_hot_linear = linear(one_hot)\n",
    "    \n",
    "    * 동작원리대로 구현한 tf keras embedding layer 선언 \n",
    "    \n",
    "    some_words = tf.constant([[3, 57, 35]]) #3번 단어 / 57번 단어 / 35번 단어로 이루어진 한 문장입니다.\n",
    "    print(\"Embedding을 진행할 문장:\", some_words.shape)\n",
    "    \n",
    "    embedding_layer = tf.keras.layers.Embedding(input_dim=64, output_dim=100)\n",
    "    #총 64개의 단어를 포함한 Embedding 레이어를 선언, 각 단어는 100차원으로 분산 표현\n",
    "    \n",
    "    Embedding 레이어는 그저 단어를 대응 시켜 줄 뿐이니 미분이 불가능. 따라서 신경망 설계를 할 때, \n",
    "    어떤 연산    결과를 Embedding 레이어에 연결시키는 것은 불가능.\n",
    "    Embedding 레이어는 입력에 직접 연결되게 사용해야 한다는 것을 꼭 기억, 그리고 그 입력은\n",
    "    원-핫 인코딩된 단어 벡터의 형태일 때가 이상적\n",
    " \n",
    "\" - embedding에서 input은 어떤 형태인가?\n",
    "    one-hot-encoding 형태, (1,n)\n",
    " - weight는 어떤 형태인가?\"\n",
    "    (단어개수, 분산차원수)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d64eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da99a48f",
   "metadata": {},
   "source": [
    "#### 19-4. RNN계열의 모델에 대해 이해할 수 있다.\n",
    "\n",
    " - RNN에서 한 레이어 안에 가중치행렬은 총 몇개가 있는지 논의해보자.\n",
    " \n",
    "   한 layer안에 1개의 가중치행렬있슴, 입력이 여러개라도 공통으로 사용함\n",
    " \n",
    " - 각 가중치를 step별로 따로 구하지 않고 하나의 가중치를 공유한다는 것이 어떤 의미인지 논의해보자.\n",
    "   \n",
    "   한개의 레이어에 입력되는 타임스텝 데이터는 시계열이라 연결속에서 맥락을 찾아야하는데,\n",
    "   가중치를 공통으로 사용해야, 연결된 한문장의 맥락파악가능\n",
    " \n",
    " - lstm에서 activation function으로 sigmoid, tanh를 사용하는 것의 의미에 대해 설명해보자.\n",
    " \n",
    "   sigmoid는 정보를 버릴지,유지할지에 사용되며, tanh는 정보를 -1~ +1로 확장시켜서,\n",
    "   미분시 최대값을 1로하여, gradient vanish를 방지함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae98464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "529be672",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863920e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90b73bd7",
   "metadata": {},
   "source": [
    "### LMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9f101b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab = {      # 사용할 단어 사전 정의\n",
    "    \"i\": 0,\n",
    "    \"need\": 1,\n",
    "    \"some\": 2,\n",
    "    \"more\": 3,\n",
    "    \"coffee\": 4,\n",
    "    \"cake\": 5,\n",
    "    \"cat\": 6,\n",
    "    \"dog\": 7\n",
    "}\n",
    "\n",
    "sentence = \"i i i i need some more coffee coffee coffee\"\n",
    "# 위 sentence\n",
    "_input = [vocab[w] for w in sentence.split()]  # [0, 0, 0, 0, 1, 2, 3, 4, 4, 4]\n",
    "\n",
    "vocab_size = len(vocab)   # 8\n",
    "\n",
    "one_hot = tf.one_hot(_input, vocab_size)\n",
    "print(one_hot.numpy())    # 원-핫 인코딩 벡터를 출력해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f5ea477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Weight\n",
      "[[ 5.3536046e-01 -4.1420171e-01]\n",
      " [-3.6139846e-02 -3.8552007e-01]\n",
      " [ 2.3436546e-04 -2.1251082e-02]\n",
      " [-6.4531159e-01 -5.9337801e-01]\n",
      " [ 3.1399584e-01  4.0389800e-01]\n",
      " [ 1.6851038e-01  6.6618919e-03]\n",
      " [-6.4044237e-01  2.3911893e-01]\n",
      " [ 7.0482564e-01  9.1578245e-02]]\n",
      "\n",
      "One-Hot Linear Result\n",
      "[[ 5.3536046e-01 -4.1420171e-01]\n",
      " [ 5.3536046e-01 -4.1420171e-01]\n",
      " [ 5.3536046e-01 -4.1420171e-01]\n",
      " [ 5.3536046e-01 -4.1420171e-01]\n",
      " [-3.6139846e-02 -3.8552007e-01]\n",
      " [ 2.3436546e-04 -2.1251082e-02]\n",
      " [-6.4531159e-01 -5.9337801e-01]\n",
      " [ 3.1399584e-01  4.0389800e-01]\n",
      " [ 3.1399584e-01  4.0389800e-01]\n",
      " [ 3.1399584e-01  4.0389800e-01]]\n"
     ]
    }
   ],
   "source": [
    "distribution_size = 2   # 보기 좋게 2차원으로 분산 표현하도록 하죠!\n",
    "linear = tf.keras.layers.Dense(units=distribution_size, use_bias=False)\n",
    "one_hot_linear = linear(one_hot)\n",
    "\n",
    "print(\"Linear Weight\")\n",
    "print(linear.weights[0].numpy())\n",
    "\n",
    "print(\"\\nOne-Hot Linear Result\")\n",
    "print(one_hot_linear.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4cf515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding을 진행할 문장: (1, 3)\n",
      "Embedding된 문장: (1, 3, 100)\n",
      "Embedding Layer의 Weight 형태: (64, 100)\n"
     ]
    }
   ],
   "source": [
    "some_words = tf.constant([[3, 57, 35]])\n",
    "# 3번 단어 / 57번 단어 / 35번 단어로 이루어진 한 문장입니다.\n",
    "\n",
    "print(\"Embedding을 진행할 문장:\", some_words.shape)\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=64, output_dim=100)\n",
    "# 총 64개의 단어를 포함한 Embedding 레이어를 선언할 것이고,\n",
    "# 각 단어는 100차원으로 분산 표현 할 것입니다.\n",
    "\n",
    "print(\"Embedding된 문장:\", embedding_layer(some_words).shape)\n",
    "print(\"Embedding Layer의 Weight 형태:\", embedding_layer.weights[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0d4b50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN에 입력할 문장: What time is it ?\n",
      "Embedding을 위해 단어 매핑: [[2 3 0 1 4]]\n",
      "입력 문장 데이터 형태: (1, 5)\n",
      "\n",
      "Embedding 결과: (1, 5, 100) tf.Tensor(\n",
      "[[[ 4.47675325e-02 -4.89621423e-02  2.61728056e-02 -2.23886613e-02\n",
      "   -1.60567872e-02 -3.84949446e-02 -2.30492596e-02  2.44883783e-02\n",
      "    1.50669478e-02  1.30547211e-03 -1.35015622e-02 -6.30959123e-03\n",
      "    2.49205939e-02  1.64047964e-02 -3.63581181e-02 -2.78396737e-02\n",
      "   -1.79786198e-02  2.44670250e-02  4.32840474e-02 -2.69526839e-02\n",
      "    3.27786691e-02  3.85920741e-02 -4.56330776e-02  4.32411470e-02\n",
      "    4.73825969e-02 -9.01911408e-03 -4.34663072e-02 -2.82626040e-02\n",
      "    4.70384248e-02 -4.17371392e-02  4.82163914e-02  3.59583981e-02\n",
      "    1.45195797e-03  4.41539921e-02  6.68396801e-03  3.95506881e-02\n",
      "    2.87267901e-02  3.92771401e-02 -2.75660679e-03  4.46359031e-02\n",
      "    5.74203581e-03 -3.31197381e-02 -3.15295458e-02  4.86161746e-02\n",
      "    3.98019888e-02 -1.99581739e-02  1.62242539e-02  3.34001817e-02\n",
      "    2.28788368e-02  2.16366090e-02  2.08383910e-02 -3.32360156e-02\n",
      "   -4.55278158e-03  5.41095808e-03 -4.01166566e-02  8.00683349e-03\n",
      "   -2.91706566e-02  2.20999867e-03  2.04457156e-02  3.76586951e-02\n",
      "    2.90800072e-02 -4.11910042e-02 -2.14975011e-02  4.07868512e-02\n",
      "   -1.74753182e-02 -3.85698900e-02 -5.18870354e-03  2.90841945e-02\n",
      "    3.01843993e-02 -4.13725972e-02  1.49335973e-02  1.16027705e-02\n",
      "    3.33254412e-03  3.59511860e-02  2.52916478e-02  2.43454091e-02\n",
      "    2.81021483e-02  3.79029661e-03 -4.87453006e-02 -2.85882354e-02\n",
      "   -3.78225558e-02 -4.98292446e-02 -2.12737173e-03  1.31899454e-02\n",
      "    2.05936767e-02 -4.16851155e-02 -2.97808778e-02  4.00752537e-02\n",
      "    4.66286205e-02 -2.85512563e-02  4.54693474e-02  4.23173048e-02\n",
      "   -1.98151823e-02 -3.30702662e-02  1.69284977e-02 -1.00449696e-02\n",
      "   -3.13829072e-02 -1.01342425e-02 -3.99313681e-02  4.12089117e-02]\n",
      "  [-1.56836398e-02  3.84977125e-02 -4.21108231e-02  4.71165664e-02\n",
      "   -2.51863599e-02  1.56035684e-02  7.86757469e-03  4.51407172e-02\n",
      "    4.96587791e-02 -1.17156655e-03 -4.62341197e-02  1.16086118e-02\n",
      "   -3.07369977e-04  1.47900917e-02  4.67211120e-02 -8.54261965e-03\n",
      "   -3.04929968e-02 -3.38913351e-02  4.82094996e-02  9.04635340e-03\n",
      "    1.50299333e-02  2.58777291e-03  3.21733095e-02  3.04886810e-02\n",
      "    2.30996870e-02 -7.63659552e-03  2.03893334e-03  1.45514049e-02\n",
      "    3.21874730e-02  4.76223268e-02 -3.21804173e-02 -4.23574336e-02\n",
      "    4.04281653e-02 -1.27068162e-02 -1.11935027e-02 -3.95796187e-02\n",
      "    4.37872149e-02 -3.59495394e-02 -6.67802244e-03 -2.89857145e-02\n",
      "   -3.60175967e-02 -2.66464949e-02 -4.63872068e-02 -1.03197247e-03\n",
      "   -4.04992327e-02  3.22633125e-02 -1.70292631e-02 -2.43086219e-02\n",
      "    1.15121007e-02  4.22084071e-02 -4.02503125e-02 -4.18133847e-02\n",
      "   -7.29210302e-03  2.78539993e-02 -4.64956760e-02 -3.79650369e-02\n",
      "    1.06565468e-02 -2.14538723e-03 -6.11394644e-03  8.10138881e-05\n",
      "   -4.66654785e-02 -4.80152294e-03 -4.13015597e-02 -3.94455791e-02\n",
      "    3.49990278e-03  3.15963291e-02 -4.83023636e-02 -3.92523631e-02\n",
      "    1.93949006e-02 -2.48305444e-02 -2.98055168e-02  2.61283778e-02\n",
      "    4.33057658e-02  3.91606204e-02  1.88236125e-02 -2.37351190e-02\n",
      "    1.40672661e-02 -1.60559304e-02 -1.03791840e-02 -3.76084559e-02\n",
      "    4.87549230e-03  2.24136598e-02 -1.59649961e-02  3.11901234e-02\n",
      "   -3.60673666e-02  3.72183807e-02  4.06255610e-02  5.60238212e-03\n",
      "    8.88121128e-03 -2.31660362e-02  3.34700681e-02  9.20748711e-03\n",
      "    5.66508621e-03  2.11496092e-02  2.40007304e-02 -3.12935859e-02\n",
      "   -4.49141264e-02 -3.76243480e-02 -2.08254810e-02 -3.59287038e-02]\n",
      "  [ 4.22916673e-02  4.10559662e-02 -2.95032188e-03 -3.59665379e-02\n",
      "   -7.79197365e-03 -3.81380804e-02  1.17134303e-04 -3.98038402e-02\n",
      "   -4.48531173e-02  4.46720980e-02 -5.62393665e-03  5.24919108e-03\n",
      "   -8.52400064e-03 -1.33054145e-02 -2.51903776e-02 -3.58768702e-02\n",
      "    5.36351278e-03 -4.01908644e-02 -9.23522562e-03  1.53453834e-02\n",
      "    1.13777742e-02  2.74642967e-02 -3.93482074e-02  1.73248760e-02\n",
      "   -2.19055656e-02 -4.34032939e-02 -2.55447030e-02  2.17881463e-02\n",
      "   -4.63987105e-02 -2.80204173e-02  2.23173238e-02 -1.77007914e-03\n",
      "    2.04608478e-02 -3.98492441e-02  6.20316342e-03  1.88119747e-02\n",
      "    4.92444374e-02  1.13228336e-02 -1.90881733e-02 -2.08975077e-02\n",
      "    2.61870734e-02 -4.18483019e-02  3.46758105e-02 -2.35877633e-02\n",
      "   -8.73538107e-03 -3.04167271e-02 -4.56009023e-02  2.22718827e-02\n",
      "    4.47088219e-02  1.58598274e-03  1.45017616e-02  2.84094475e-02\n",
      "   -2.31697913e-02  3.80722992e-02 -3.03342827e-02 -5.26482984e-03\n",
      "    2.30991580e-02 -4.58160043e-02 -4.97441776e-02  2.85303034e-02\n",
      "   -2.00417638e-02 -3.99089344e-02 -3.73860449e-03 -9.18943807e-03\n",
      "   -1.64011940e-02  4.38062064e-02 -2.47553121e-02 -1.52904019e-02\n",
      "    1.21567026e-02 -4.43608686e-03  3.31458710e-02 -6.17156178e-03\n",
      "   -2.90818214e-02  4.47391979e-02 -1.36655569e-03  4.12624516e-02\n",
      "    1.88640244e-02  4.06954437e-03 -4.58095223e-03  5.87619469e-03\n",
      "   -2.72929668e-03 -1.05086565e-02 -2.56164912e-02  4.23059799e-02\n",
      "    3.21692936e-02 -4.70151789e-02  4.17695753e-02  3.87253277e-02\n",
      "   -1.82037428e-03  4.63241823e-02  8.48894194e-03 -1.16375796e-02\n",
      "   -1.13793984e-02 -6.06181473e-03  4.12838720e-02 -4.43325751e-02\n",
      "   -3.66803519e-02  4.61803712e-02 -3.72278094e-02 -2.21736915e-02]\n",
      "  [-1.33189186e-02 -2.52622124e-02 -1.02824457e-02 -3.55748422e-02\n",
      "    3.19227241e-02 -3.05945519e-02 -4.13655639e-02 -1.09979883e-02\n",
      "   -4.39262390e-03  1.06649511e-02  2.88595892e-02 -2.02960856e-02\n",
      "    2.37880982e-02 -2.03762781e-02  3.48495506e-02  4.14297692e-02\n",
      "   -3.37808356e-02 -2.90188938e-03  1.00387260e-03 -4.35874723e-02\n",
      "    6.23401254e-03 -1.07689016e-02  4.17109840e-02 -3.92424688e-02\n",
      "   -4.99256253e-02  3.40145566e-02  1.00525133e-02  4.08465602e-02\n",
      "   -5.89960814e-03  3.84389237e-03  6.50737435e-03 -3.35879475e-02\n",
      "    4.08756398e-02  4.40550111e-02  1.75586082e-02  2.09010765e-03\n",
      "   -2.54332554e-02  4.77148220e-03 -5.77874109e-03  4.79748510e-02\n",
      "    4.97740023e-02  2.06887722e-04  2.94788927e-03  3.23617794e-02\n",
      "    1.77519061e-02  1.56788938e-02  4.92837541e-02 -2.96713356e-02\n",
      "   -4.80520986e-02  3.13786156e-02 -1.92922242e-02  4.57954146e-02\n",
      "   -1.86827667e-02  1.26981847e-02 -2.71302219e-02  1.54354014e-02\n",
      "    2.85689346e-02  1.12066977e-02  1.33736022e-02 -4.57398556e-02\n",
      "   -1.32091530e-02 -4.99047339e-04  4.32147831e-03  1.27816685e-02\n",
      "   -2.55503654e-02 -2.15878487e-02  4.61008586e-02 -4.29205783e-02\n",
      "   -1.93887837e-02 -4.10547853e-02 -1.71068795e-02 -2.09216606e-02\n",
      "    1.49445198e-02  2.05148123e-02 -2.46208552e-02  2.24359669e-02\n",
      "   -2.87229307e-02 -4.74409238e-02 -2.95367092e-03 -1.46984458e-02\n",
      "   -2.53975391e-02  4.32194360e-02  2.60166079e-03  3.45819257e-02\n",
      "    3.58452685e-02 -4.06700000e-02 -2.70183571e-02  1.30284317e-02\n",
      "   -8.61771405e-05 -3.25465687e-02  6.03630394e-03  1.46267153e-02\n",
      "   -3.11590917e-02  7.03848526e-03  2.10413449e-02  3.48760150e-02\n",
      "   -2.60766037e-02  4.47822474e-02  4.05536331e-02  4.50058319e-02]\n",
      "  [ 4.67314981e-02  2.40631811e-02 -6.96867704e-03 -9.84653085e-03\n",
      "    1.98966153e-02  2.16161087e-03 -2.66957171e-02  3.18905227e-02\n",
      "    7.43212551e-03 -1.16026774e-02  1.95852183e-02  8.96244124e-03\n",
      "    8.77883285e-03 -3.59235778e-02  1.97173245e-02  3.82358469e-02\n",
      "   -1.58259161e-02 -4.63748090e-02  6.65275007e-03  4.24823500e-02\n",
      "    4.22145016e-02 -4.64145206e-02 -2.24975348e-02  1.95859931e-02\n",
      "    2.77125835e-03 -8.73650238e-03  3.10162455e-03 -1.27322078e-02\n",
      "   -2.62065176e-02  3.14625390e-02  8.68725777e-03 -1.92368273e-02\n",
      "   -3.73803265e-02 -2.74111032e-02  4.85779084e-02  4.50796969e-02\n",
      "   -5.39505482e-03  1.25761740e-02  2.22939290e-02  6.18941709e-03\n",
      "   -3.74068618e-02 -1.34559870e-02 -6.36877865e-03  2.69720443e-02\n",
      "   -2.17413902e-03 -2.82291062e-02  6.89722598e-04  4.63977344e-02\n",
      "   -4.55089323e-02  6.48440048e-03 -4.01789546e-02 -3.13506722e-02\n",
      "    4.53596972e-02 -3.80787626e-02  2.09322087e-02  2.34455802e-02\n",
      "   -3.16604748e-02  1.21823065e-02  3.85313295e-02  5.23388386e-04\n",
      "   -4.12281752e-02  1.56286396e-02  2.30818875e-02 -1.79582611e-02\n",
      "   -4.57683094e-02  1.36802457e-02  1.41405575e-02  1.44077651e-02\n",
      "   -6.07134029e-03 -1.10849962e-02 -1.77552812e-02  2.20268629e-02\n",
      "    4.87952270e-02 -4.21606787e-02 -1.30708218e-02  1.94402449e-02\n",
      "    4.64720018e-02 -4.61528189e-02  2.22069137e-02  1.52671598e-02\n",
      "   -1.25339031e-02 -1.29861124e-02 -1.40588768e-02  1.96289085e-02\n",
      "   -8.97014141e-03  4.15338986e-02  2.26506330e-02 -9.02730227e-03\n",
      "   -1.95122007e-02 -1.50635131e-02 -4.13003340e-02 -5.97164780e-03\n",
      "   -2.29787081e-04  3.45886610e-02 -3.38932760e-02  1.30146258e-02\n",
      "   -2.92822253e-02  3.77290510e-02 -2.49376893e-02  2.48068087e-02]]], shape=(1, 5, 100), dtype=float32)\n",
      "Embedding Layer의 Weight 형태: (5, 100)\n",
      "\n",
      "RNN 결과 (모든 Step Output): (1, 5, 64)\n",
      "RNN Layer의 Weight 형태: (100, 64)\n",
      "\n",
      "RNN 결과 (최종 Step Output): (1, 64)\n",
      "RNN Layer의 Weight 형태: (100, 64)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"What time is it ?\"\n",
    "dic = {\n",
    "    \"is\": 0,\n",
    "    \"it\": 1,\n",
    "    \"What\": 2,\n",
    "    \"time\": 3,\n",
    "    \"?\": 4\n",
    "}\n",
    "\n",
    "print(\"RNN에 입력할 문장:\", sentence)\n",
    "\n",
    "sentence_tensor = tf.constant([[dic[word] for word in sentence.split()]])\n",
    "\n",
    "print(\"Embedding을 위해 단어 매핑:\", sentence_tensor.numpy())\n",
    "print(\"입력 문장 데이터 형태:\", sentence_tensor.shape)\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=len(dic), output_dim=100)\n",
    "emb_out = embedding_layer(sentence_tensor)\n",
    "\n",
    "print(\"\\nEmbedding 결과:\", emb_out.shape, emb_out)\n",
    "print(\"Embedding Layer의 Weight 형태:\", embedding_layer.weights[0].shape)\n",
    "\n",
    "rnn_seq_layer = \\\n",
    "tf.keras.layers.SimpleRNN(units=64, return_sequences=True, use_bias=False)\n",
    "rnn_seq_out = rnn_seq_layer(emb_out)\n",
    "\n",
    "print(\"\\nRNN 결과 (모든 Step Output):\", rnn_seq_out.shape)\n",
    "print(\"RNN Layer의 Weight 형태:\", rnn_seq_layer.weights[0].shape)\n",
    "\n",
    "rnn_fin_layer = tf.keras.layers.SimpleRNN(units=64, use_bias=False)\n",
    "rnn_fin_out = rnn_fin_layer(emb_out)\n",
    "\n",
    "print(\"\\nRNN 결과 (최종 Step Output):\", rnn_fin_out.shape)\n",
    "print(\"RNN Layer의 Weight 형태:\", rnn_fin_layer.weights[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e396ec24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSTM 결과 (모든 Step Output): (1, 5, 64)\n",
      "LSTM Layer의 Weight 형태: (100, 256)\n",
      "\n",
      "LSTM 결과 (최종 Step Output): (1, 64)\n",
      "LSTM Layer의 Weight 형태: (100, 256)\n"
     ]
    }
   ],
   "source": [
    "lstm_seq_layer = tf.keras.layers.LSTM(units=64, return_sequences=True, use_bias=False)\n",
    "lstm_seq_out = lstm_seq_layer(emb_out)\n",
    "\n",
    "print(\"\\nLSTM 결과 (모든 Step Output):\", lstm_seq_out.shape)\n",
    "print(\"LSTM Layer의 Weight 형태:\", lstm_seq_layer.weights[0].shape)\n",
    "\n",
    "lstm_fin_layer = tf.keras.layers.LSTM(units=64, use_bias=False)\n",
    "lstm_fin_out = lstm_fin_layer(emb_out)\n",
    "\n",
    "print(\"\\nLSTM 결과 (최종 Step Output):\", lstm_fin_out.shape)\n",
    "print(\"LSTM Layer의 Weight 형태:\", lstm_fin_layer.weights[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59efcf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 문장 데이터 형태: (1, 5, 100)\n",
      "Bidirectional RNN 결과 (최종 Step Output): (1, 5, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sentence = \"What time is it ?\"\n",
    "dic = {\n",
    "    \"is\": 0,\n",
    "    \"it\": 1,\n",
    "    \"What\": 2,\n",
    "    \"time\": 3,\n",
    "    \"?\": 4\n",
    "}\n",
    "\n",
    "sentence_tensor = tf.constant([[dic[word] for word in sentence.split()]])\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=len(dic), output_dim=100)\n",
    "emb_out = embedding_layer(sentence_tensor)\n",
    "\n",
    "print(\"입력 문장 데이터 형태:\", emb_out.shape)\n",
    "\n",
    "bi_rnn = \\\n",
    "tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.SimpleRNN(units=64, use_bias=False, return_sequences=True)\n",
    ")\n",
    "bi_out = bi_rnn(emb_out)\n",
    "\n",
    "print(\"Bidirectional RNN 결과 (최종 Step Output):\", bi_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51873a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10e6ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708f9ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5fc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc8fd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90540b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
