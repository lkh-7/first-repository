{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2edb6da",
   "metadata": {},
   "source": [
    "## 8-7. 프로젝트: 한영 번역기 만들기\n",
    "#### 라이브러리 버전을 확인해 봅니다\n",
    "\n",
    "사용할 라이브러리 버전을 둘러봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7cb82ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.3\n",
      "2.6.0\n",
      "3.4.3\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import tensorflow\n",
    "import matplotlib\n",
    "\n",
    "print(pandas.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04f53ff",
   "metadata": {},
   "source": [
    "#### Step 1. 데이터 다운로드\n",
    "아래 링크에서 korean-english-park.train.tar.gz 를 다운로드받아 한영 병렬 데이터를 확보합니다.\n",
    "\n",
    "* [jungyeul/korean-parallel-corpora](https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3118aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time,copy\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d6e4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/s2s_translation/kor_eng/korean-english-park.train.tar.gz\n"
     ]
    }
   ],
   "source": [
    "path = r'/aiffel/aiffel/s2s_translation/kor_eng'\n",
    "url = 'https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.train.tar.gz'\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    path + r'/korean-english-park.train.tar.gz',\n",
    "    origin= url,\n",
    "    extract=True, cache_subdir='data', cache_dir= path )\n",
    "    \n",
    "print(path_to_zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee8eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f61d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/s2s_translation/kor_eng/data/korean-english-park.train.ko\n",
      "/aiffel/aiffel/s2s_translation/kor_eng/data/korean-english-park.train.en\n",
      "kor Data Size: 94123 eng Data Size: 94123\n",
      "kor_Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n",
      "eng_Example:\n",
      ">> Much of personal computing is about \"can you top this?\"\n",
      ">> Amid mounting pressure on North Korea to abandon its nuclear weapons program Japanese and North Korean diplomats have resumed talks on normalizing diplomatic relations.\n",
      ">> “Guard robots are used privately and professionally to detect intruders or fire,” Karlsson said.\n",
      ">> Authorities from the Water Resources Ministry plan to begin construction next year on the controversial and hugely expensive project.\n",
      ">> Researchers also have debated whether weight-training has a big impact on the heart, since it does not give the heart and lungs the kind of workout they get from aerobic activities such as brisk walking or running for at least 20 minutes.\n",
      "kor_Example2:\n",
      ">> 마이크로소프트社의 공동 창업자인 빌 게이츠는 알람 시계와 펜과 같은 일상 생활용품에 지능형 컴퓨터 기술을 결합시키는 전략을 발표하면서, 첨단 기술 산업의 침체에도 불구하고 혁신은 계속 되고 있다고 말했다.\n",
      ">> 제 23차 연례 컴덱스 박람회의 개회사를 한 케이츠는 2년여전 기술 산업의 거품이 붕괴된 이후에 첨단 기술에 대해 부정적인 인식이 있다고 말했다.\n",
      "eng_Example2:\n",
      ">> despite the high-tech industry's downturn, co-founder Bill Gates said as he unveiled a strategy to push \"smart\" computing technology into everyday gadgets such as alarm clocks and pens.\n",
      ">> Gates, who opened the 23rd annual Comdex trade show, said there was a negative perception of high tech following the collapse of the tech bubble about two years ago.\n"
     ]
    }
   ],
   "source": [
    "path_to_file_kor = os.path.dirname(path_to_zip)+ '/data/korean-english-park.train.ko'\n",
    "print(path_to_file_kor)\n",
    "path_to_file_eng = os.path.dirname(path_to_zip)+ '/data/korean-english-park.train.en'\n",
    "print(path_to_file_eng)\n",
    "\n",
    "with open(path_to_file_kor, \"r\") as f:    \n",
    "    raw_kor = f.read().splitlines()\n",
    "    \n",
    "with open(path_to_file_eng, \"r\") as f:    \n",
    "    raw_eng = f.read().splitlines()\n",
    "    \n",
    "print(\"kor Data Size:\", len(raw_kor), \"eng Data Size:\", len(raw_eng))   #94123,  94123\n",
    "\n",
    "print(\"kor_Example:\")\n",
    "for sen in raw_kor[0:100][::20]: print(\">>\", sen)\n",
    "print(\"eng_Example:\")\n",
    "for sen in raw_eng[0:100][::20]: print(\">>\", sen) \n",
    "    \n",
    "    \n",
    "print(\"kor_Example2:\")\n",
    "for sen in raw_kor[99:101]: print(\">>\", sen)\n",
    "print(\"eng_Example2:\")\n",
    "for sen in raw_eng[99:101]: print(\">>\", sen)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f172e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd7b13c2",
   "metadata": {},
   "source": [
    "#### Step 2. 데이터 정제\n",
    "set 데이터형이 **중복을 허용하지 않는다는 것을 활용**해 중복된 데이터를 제거하도록 합니다. 데이터의 **병렬 쌍이 흐트러지지 않게 주의**하세요! 중복을 제거한 데이터를 cleaned_corpus 에 저장합니다.\n",
    "\n",
    "앞서 정의한 preprocessing() 함수는 한글에서는 동작하지 않습니다. **한글에 적용할 수 있는 정규식**을 추가하여 함수를 재정의하세요!\n",
    "\n",
    "타겟 언어인 영문엔 <start> 토큰과 <end> 토큰을 추가하고 split() 함수를 이용하여 토큰화합니다. 한글 토큰화는 **KoNLPy** 의 mecab 클래스를 사용합니다.\n",
    "\n",
    "모든 데이터를 사용할 경우 학습에 굉장히 오랜 시간이 걸립니다. cleaned_corpus로부터 **토큰의 길이가 40 이하인 데이터를 선별**하여 eng_corpus와 kor_corpus를 각각 구축하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd992e",
   "metadata": {},
   "source": [
    "#### raw_kor, raw_eng 를 set 및 zip을 활용하여, 병렬 쌍이 흐트러지지 않게해서  중복제거후,  cleaned_corpus 에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe04d26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94123 94123\n",
      "cleaned_corpus ['버지니아 공대 4학년 학생인 셰인 무어는 17일(현지시간) 조와 3년 전 점심을 같이 먹은 적이 있다고 말했다.    Virginia Tech senior Shane Moore said Tuesday he recalled having lunch with Cho three years ago.\\n', '#47484 보여주어야 한다고 말했다. 특히 그는 북한이 \"\"슈퍼노트를 제조할 수 있는 장비와 동판을 제거했다는 증거를 제시해야 앞으로 지폐를 제작할 수 있는 능력이 있다는 염려가 줄어들 것이다\"\"라고 말했다.    Specifically, he said, the North must \"\"provide evidence that the equipment and plates for the so-called supernotes had been destroyed so that concerns about further ability [to print more notes] will be reduced.\\n'] 78968\n",
      "저장된 cleaned_corpus 로드후 확인:  ['버지니아 공대 4학년 학생인 셰인 무어는 17일(현지시간) 조와 3년 전 점심을 같이 먹은 적이 있다고 말했다.    Virginia Tech senior Shane Moore said Tuesday he recalled having lunch with Cho three years ago.', '#47484 보여주어야 한다고 말했다. 특히 그는 북한이 \"\"슈퍼노트를 제조할 수 있는 장비와 동판을 제거했다는 증거를 제시해야 앞으로 지폐를 제작할 수 있는 능력이 있다는 염려가 줄어들 것이다\"\"라고 말했다.    Specifically, he said, the North must \"\"provide evidence that the equipment and plates for the so-called supernotes had been destroyed so that concerns about further ability [to print more notes] will be reduced.']\n",
      "중복제거후 문장개수:  78968\n"
     ]
    }
   ],
   "source": [
    "## 데이터 정제 및 토큰화\n",
    "assert len(raw_kor) == len(raw_eng)\n",
    "print(len(raw_kor), len(raw_eng))    \n",
    "cleaned_corpus = list(set(zip(raw_kor,raw_eng)))\n",
    "\n",
    "# 문장별로 튜플을 풀어서 str 로 전환: 한개의 문장내 한글 vs 영어는 공백4칸으로 구분, 문장별 구분은 '\\n'으로 구분\n",
    "cleaned_corpus = [line[0] + ' '*4 + line[1] +'\\n' for line in cleaned_corpus]\n",
    "print(\"cleaned_corpus\",cleaned_corpus[:2], len(cleaned_corpus))\n",
    "\n",
    "# 저장\n",
    "with open(os.path.dirname(path_to_zip)+ '/cleaned_corpus/kor_eng_pairs.txt', 'w') as f:  \n",
    "        for line in cleaned_corpus:\n",
    "            f.write(line)\n",
    "            \n",
    "# 저장여부 확인\n",
    "with open(os.path.dirname(path_to_zip)+ '/cleaned_corpus/kor_eng_pairs.txt', \"r\") as f:    \n",
    "    cleaned_corpus_check = f.read().splitlines()    # \\n 떼어 놓고, line별 구분됨 \n",
    "    \n",
    "print(\"저장된 cleaned_corpus 로드후 확인: \", cleaned_corpus_check[:2])\n",
    "print(\"중복제거후 문장개수: \", len(cleaned_corpus))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af502b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복제거이전 총문장개수: 78968\n",
      "(78968, 1)                                       cleaned_corpus\n",
      "0  버지니아 공대 4학년 학생인 셰인 무어는 17일(현지시간) 조와 3년 전 점심을 같...\n",
      "1  #47484 보여주어야 한다고 말했다. 특히 그는 북한이 \"\"슈퍼노트를 제조할 수 ...\n",
      "2  최근 FARC로부터 석방된 한 피랍자는 베탕쿠르의 건강이 매우 악화됐다고 진술했으며...\n",
      "3  식품의약품안전청은 양식 민물고기에서 위험 수위에 이르는 발암 물질이 검출되었다는 해...\n",
      "4  폭스 뉴스가 입수한 법원 문서에 의하면 잭슨이 경매 등록일인 내달 19일까지 빚을 ...\n",
      "중복문장개수: 0\n",
      "Empty DataFrame\n",
      "Columns: [cleaned_corpus]\n",
      "Index: []\n",
      "중복제거이후 Unique 문장개수: 78968\n",
      "중복제거 비율 % 0.0\n"
     ]
    }
   ],
   "source": [
    "## 중복여부 다시 체크: DataFrame Duplicate로 체크\n",
    "\n",
    "print(\"중복제거이전 총문장개수:\",len(cleaned_corpus))\n",
    "\n",
    "# 중복제거위해 DataFrame만들기:  \n",
    "df_pairs = pd.DataFrame({'cleaned_corpus': cleaned_corpus}) \n",
    "print(df_pairs.shape, df_pairs[:5])\n",
    "\n",
    "# 중복 문장개수 체크\n",
    "print(\"중복문장개수:\",df_pairs.duplicated().sum())\n",
    "print(df_pairs[df_pairs.duplicated()==True])\n",
    "\n",
    "# 중복제거\n",
    "df_pairs_clean = df_pairs.drop_duplicates()\n",
    "\n",
    "print(\"중복제거이후 Unique 문장개수:\",len(df_pairs_clean))\n",
    "print(\"중복제거 비율 %\", round((len(df_pairs)-len(df_pairs_clean))*100/len(df_pairs),2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a9aff",
   "metadata": {},
   "source": [
    "#### preprocess_sentence 함수를 한글도 처리가능케 재정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfdc2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, kor_token=False, eng_token=False):\n",
    "    \n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^0-9ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if kor_token:\n",
    "        sentence = '<BOS> ' + sentence\n",
    "    if eng_token:\n",
    "        sentence += ' <EOS>'\n",
    "    \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de74aa",
   "metadata": {},
   "source": [
    "#### 문장을 단어단위 토큰화 단어수 40개미만 문장만 선별\n",
    "\n",
    "* 1.**한글 형태소분석기로 kor 단어토큰화 및 불용어제거 : enc_corpus**\n",
    "\n",
    "* 2.**영어는 공백에의해 단어 split됨: enc_corpus**\n",
    "\n",
    "* 3.**긴문장 과다에의한 학습시간 과다소요 방지위해서, 한글 단어수 40개이상 및 영어단어수 45개이상의 문장은 제거하고, 각각40개,45개 미만 문장만 선별: 문장수 78,968개 ==> 76,536개로 축소 >>> 한글 단어 39개 문장에 매치되는 영어단어수 91개임**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1434fd41",
   "metadata": {},
   "source": [
    "####  preprocess 및 형태소 분석 : kor_corpus, eng_corpus 분리해서 데이터 preprocess 및 형태소 분석으로 단어추출 토큰화  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e734539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0 ['버지니아', '공대', '4', '학년', '학생', '인', '셰', '인', '무어', '는', '17', '일', '현지', '시간', '조', '와', '3', '년', '전', '점심', '을', '같이', '먹', '은', '적', '이', '있', '다고', '말', '했', '다', '.'] 32 ['<BOS>', 'virginia', 'tech', 'senior', 'shane', 'moore', 'said', 'tuesday', 'he', 'recalled', 'having', 'lunch', 'with', 'cho', 'three', 'years', 'ago', '.', '<EOS>'] 19\n",
      "i 1 ['47484', '보여', '주', '어야', '한다고', '말', '했', '다', '.', '특히', '그', '는', '북한', '이', '슈퍼', '노트', '를', '제조', '할', '수', '있', '는', '장비', '와', '동판', '을', '제거', '했', '다는', '증거', '를', '제시', '해야', '앞', '으로', '지폐', '를', '제작', '할', '수', '있', '는', '능력', '이', '있', '다는', '염려', '가', '줄어들', '것', '이', '다', '라고', '말', '했', '다', '.'] 57 ['<BOS>', 'specifically', ',', 'he', 'said', ',', 'the', 'north', 'must', 'provide', 'evidence', 'that', 'the', 'equipment', 'and', 'plates', 'for', 'the', 'so', 'called', 'supernotes', 'had', 'been', 'destroyed', 'so', 'that', 'concerns', 'about', 'further', 'ability', 'to', 'print', 'more', 'notes', 'will', 'be', 'reduced', '.', '<EOS>'] 39\n",
      "i 10000 ['긴장', '한', '소매상인', '들', '은', '오늘', '아침', '이전', '보다', '일찍', '가게', '문', '을', '열', '었', '습니다', '.', '이', '대학생', '은', '댈러스', '의', '베스트', '바이', '에서', '휴대', '용', '컴퓨터', '를', '구매', '하', '기', '위해', '12', '시간', '동안', '줄', '을', '서', '있', '습니다', '.'] 42 ['<BOS>', 'nervous', 'retailers', 'opened', 'earlier', 'than', 'ever', 'this', 'morning', 'this', 'college', 'student', 'waiting', 'in', 'line', 'twelve', 'hours', 'to', 'grab', 'a', 'laptop', 'at', 'a', 'best', 'buy', 'in', 'dallas', '<EOS>'] 28\n",
      "i 20000 ['대사관', '관계자', '는', '감바리', '유엔', '특사', '가', '군부', '지도자', '탄', '슈에', '장군', '과', '의', '회담', '대신', '동남아시아', '와', '유럽연합', '과', '의', '관계', '에', '대한', '세미나', '에', '참석', '하', '기', '위해', '미얀마', '북부', '샨', '지역', '을', '방문', '했', '다고', '전했', '다', '.'] 41 ['<BOS>', 'the', 'associated', 'press', ',', 'citing', 'diplomats', ',', 'said', 'gambari', 'was', 'taken', 'on', 'a', 'government', 'sponsored', 'trip', 'to', 'attend', 'a', 'seminar', 'in', 'the', 'far', 'northern', 'shan', 'state', 'on', 'eu', 'relations', 'with', 'southeast', 'asia', ',', 'instead', 'of', 'meeting', 'with', 'junta', 'leader', 'senior', 'gen', '.', 'than', 'shwe', '.', '<EOS>'] 47\n",
      "kor_text_data [['버지니아', '공대', '4', '학년', '학생', '인', '셰', '인', '무어', '는', '17', '일', '현지', '시간', '조', '와', '3', '년', '전', '점심', '을', '같이', '먹', '은', '적', '이', '있', '다고', '말', '했', '다', '.'], ['47484', '보여', '주', '어야', '한다고', '말', '했', '다', '.', '특히', '그', '는', '북한', '이', '슈퍼', '노트', '를', '제조', '할', '수', '있', '는', '장비', '와', '동판', '을', '제거', '했', '다는', '증거', '를', '제시', '해야', '앞', '으로', '지폐', '를', '제작', '할', '수', '있', '는', '능력', '이', '있', '다는', '염려', '가', '줄어들', '것', '이', '다', '라고', '말', '했', '다', '.'], ['최근', 'farc', '로부터', '석방', '된', '한', '피랍', '자', '는', '베탕쿠르', '의', '건강', '이', '매우', '악화', '됐', '다고', '진술', '했으며', '일부', '언론', '은', '그', '가', '피랍', '된', '지', '6', '주년', '이', '되', '는', '지난', '2', '월', '23', '일', '부터', '단식', '투쟁', '을', '시작', '했', '다고', '보도', '했', '다', '.'], ['식품', '의', '약', '품안', '전청', '은', '양식', '민물고기', '에서', '위험', '수위', '에', '이르', '는', '발암', '물질', '이', '검출', '되', '었', '다는', '해양수산부', '의', '발표', '가', '있', '자', '양식', '어류', '두', '종', '에', '대해', '긴급', '수거', '명령', '을', '내렸', '다', '.'], ['폭스', '뉴스', '가', '입수', '한', '법원', '문서', '에', '의하', '면', '잭슨', '이', '경매', '등록', '일', '인', '내달', '19', '일', '까지', '빚', '을', '갚', '지', '않', '으면', '부동산', '이', '경매', '에', '부쳐진다', '.']] 30000\n",
      "eng_text_data [['<BOS>', 'virginia', 'tech', 'senior', 'shane', 'moore', 'said', 'tuesday', 'he', 'recalled', 'having', 'lunch', 'with', 'cho', 'three', 'years', 'ago', '.', '<EOS>'], ['<BOS>', 'specifically', ',', 'he', 'said', ',', 'the', 'north', 'must', 'provide', 'evidence', 'that', 'the', 'equipment', 'and', 'plates', 'for', 'the', 'so', 'called', 'supernotes', 'had', 'been', 'destroyed', 'so', 'that', 'concerns', 'about', 'further', 'ability', 'to', 'print', 'more', 'notes', 'will', 'be', 'reduced', '.', '<EOS>'], ['<BOS>', 'hostages', 'whom', 'the', 'rebel', 'group', 'recently', 'freed', 'said', 'she', 'is', 'in', 'poor', 'health', ',', 'and', 'other', 'reports', 'have', 'said', 'she', 'began', 'a', 'hunger', 'strike', 'february', '23', ',', 'the', 'sixth', 'anniversary', 'of', 'her', 'captivity', '.', 'so', ',', 'you', 'who', 'lead', 'farc', ',', 'you', 'now', 'have', 'a', 'date', 'with', 'history', ',', 'sarkozy', 'warned', '.', '<EOS>'], ['<BOS>', 'the', 'food', 'and', 'drug', 'administration', 'ordered', 'an', 'immediate', 'recall', 'of', 'two', 'species', 'of', 'farmed', 'fish', 'in', 'markets', 'here', ',', 'less', 'than', 'a', 'day', 'after', 'the', 'ministry', 'of', 'maritime', 'affairs', 'and', 'fisheries', 'said', 'that', 'a', 'carcinogen', 'had', 'been', 'found', 'at', 'harmful', 'levels', 'in', 'freshwater', 'fish', 'raised', 'in', 'captivity', '.', '<EOS>'], ['<BOS>', 'court', 'documents', 'obtained', 'by', 'fox', 'news', 'warn', 'jackson', 'that', 'he', 'has', 'until', 'the', 'date', 'of', 'the', 'auction', 'to', 'take', 'action', 'to', 'keep', 'his', 'lavish', 'los', 'olivos', 'estate', '.', '<EOS>']] 30000\n",
      "kor_corpus 길이:  30000\n",
      "eng_corpus 길이:  30000\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab \n",
    "\n",
    "tokenizer = Mecab()\n",
    "\n",
    "# 형태소 분석, 토큰화(tokenization)\n",
    "kor_text_data = []\n",
    "eng_text_data = []\n",
    "\n",
    "# 샘플수 제한할 경우 \n",
    "num_examples = 30000  # len(cleaned_corpus)78968 \n",
    "\n",
    "for i, pair in enumerate(cleaned_corpus[:num_examples]):\n",
    "    kor_temp_data = []\n",
    "    eng_temp_data = []\n",
    "    # pair 분리\n",
    "    kor_sentence = pair.split(' '*4)[0]\n",
    "    eng_sentence = pair.split(' '*4)[1]\n",
    "    #print(\"kor_sentence\",kor_sentence, \"eng_sentence\",eng_sentence)\n",
    "    \n",
    "    # preprocess: 데이터 정제화\n",
    "    kor_sentence = preprocess_sentence(kor_sentence)\n",
    "    eng_sentence  = preprocess_sentence(eng_sentence, kor_token=True, eng_token=True)\n",
    "    \n",
    "    # 토큰화\n",
    "    kor_temp_data = tokenizer.morphs(kor_sentence) \n",
    "    eng_temp_data = eng_sentence.split(' ')\n",
    "    \n",
    "    # append\n",
    "    kor_text_data.append(kor_temp_data)\n",
    "    eng_text_data.append(eng_temp_data)\n",
    "               \n",
    "    if i <= 1 or i%10000==0:\n",
    "        print(\"i\",i, kor_text_data[-1],len(kor_text_data[-1]), eng_text_data[-1], len(eng_text_data[-1]))\n",
    "    if i==len(cleaned_corpus)-1:\n",
    "        print(\"End_i\", i)\n",
    "    \n",
    "print(\"kor_text_data\",kor_text_data[:5], len(kor_text_data))   # 40037\n",
    "print(\"eng_text_data\",eng_text_data[:5], len(eng_text_data))   # 40037\n",
    "\n",
    "\n",
    "# enc_text_data 를  enc_corpus로, dec_text_data 를  dec_corpus로명칭 복원\n",
    "kor_corpus = kor_text_data.copy()\n",
    "eng_corpus = eng_text_data.copy()\n",
    "\n",
    "print(\"kor_corpus 길이: \", len(kor_corpus))\n",
    "print(\"eng_corpus 길이: \", len(eng_corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649fb502",
   "metadata": {},
   "source": [
    "#### Step 3. 데이터 토큰화\n",
    "앞서 정의한 tokenize() 함수를 사용해 데이터를 텐서로 변환하고 각각의 tokenizer를 얻으세요! 단어의 수는 실험을 통해 적당한 값을 맞춰주도록 합니다! (최소 10,000 이상!)\n",
    "\n",
    "❗ 주의: 난이도에 비해 데이터가 많지 않아 훈련 데이터와 검증 데이터를 따로 나누지는 않습니다. ===> 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd1b5f",
   "metadata": {},
   "source": [
    "#### 정수인덱스 토큰화하기, 보정처리 및 padding:  40단어 이하 문장만 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7420479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 정수인덱스 토큰화하기, 보정처리 및 padding\n",
    "def tokenize(corpus, maxlen= 39, kor = False): \n",
    "    ## 정수인덱스 토큰화\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',lower = False, oov_token= '<UNK>')  #lower 또하면 'BOS''EOS'도 소문자됨      \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    #[주의!!] 보정처리할 경우에는 texts_to_sequences(corpus)를 보정처리후 padding전에  해야함\n",
    "    #tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "       \n",
    "    ## 보정처리, texts_to_sequences 및 패딩: \n",
    "    print(\"보정전 단어사전 길이:\", len(tokenizer.index_word), len(tokenizer.word_index))\n",
    "    if kor == True:\n",
    "        # 한글 보정처리\n",
    "        print(\"보정전 tokenizer.index_word[0~10]\",[(k,v) for (k,v) in tokenizer.index_word.items() if k <=10])\n",
    "        \n",
    "        tokenizer.index_word = {k + 3: v for k,v in tokenizer.index_word.items()}\n",
    "        tokenizer.index_word.update({0:'<PAD>',1:'', 2:'', 3:'<UNK>'}) \n",
    "        del tokenizer.index_word[1]  # 1 공백제거 \n",
    "        del tokenizer.index_word[2]  # 2 공백제거\n",
    "        del tokenizer.index_word[4]  # 4 중복된 'UNK' 제거\n",
    "        tokenizer.word_index = {v: k for k,v in tokenizer.index_word.items()}\n",
    "        print(\"tokenizer.index_word\",tokenizer.index_word[0],tokenizer.index_word[3],tokenizer.index_word[5])\n",
    "        print(\"tokenizer.word_index\",tokenizer.word_index['<PAD>'],tokenizer.word_index['<UNK>'])\n",
    "        print(\"보정후 tokenizer.index_word[0~10]\",[(k,v) for i,(k,v) in enumerate(tokenizer.index_word.items()) if i <=10])\n",
    "        \n",
    "        # 보정처리완료후texts_to_sequences(corpus)\n",
    "        tensor = tokenizer.texts_to_sequences(corpus)\n",
    "        # 패딩\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen = maxlen, padding='post') #maxlen = 50,\n",
    "    else:\n",
    "        # 영어 보정처리\n",
    "        print(\"보정전 tokenizer.index_word[0~10]\",[(k,v) for i,(k,v) in enumerate(tokenizer.index_word.items()) if i <=10])\n",
    "        \n",
    "        tokenizer.index_word = {k + 3: v for k,v in tokenizer.index_word.items()}\n",
    "        tokenizer.index_word.update({0:'<PAD>',1: '<BOS>', 2:'<EOS>', 3:'<UNK>'}) \n",
    "        del tokenizer.index_word[4]  # 4 중복된 'UNK' 제거\n",
    "        del tokenizer.index_word[7]  # 7 중복된 'BOS' 제거\n",
    "        del tokenizer.index_word[8]  # 8 중복된 'EOS' 제거\n",
    "        tokenizer.word_index = {v: k for k,v in tokenizer.index_word.items()}\n",
    "        print(\"tokenizer.index_word\",tokenizer.index_word[0],tokenizer.index_word[1],tokenizer.index_word[2],\n",
    "             tokenizer.index_word[3],tokenizer.index_word[5])\n",
    "        print(\"tokenizer.word_index\",tokenizer.word_index['<PAD>'],tokenizer.word_index['<BOS>'],\n",
    "              tokenizer.word_index['<EOS>'],tokenizer.word_index['<UNK>'])    \n",
    "        print(\"보정후 tokenizer.index_word[0~10]\",[(k,v) for (k,v) in tokenizer.index_word.items() if k <=10])\n",
    "        \n",
    "        # 보정처리완료후texts_to_sequences(corpus)\n",
    "        tensor = tokenizer.texts_to_sequences(corpus)\n",
    "        # 패딩\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen = maxlen, padding='post') #maxlen = 50,\n",
    "        \n",
    "    \n",
    "    print(\"보정후 단어사전 길이:\", len(tokenizer.index_word), len(tokenizer.word_index))\n",
    "    \n",
    "    return tensor, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ebf1a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kor_max 127\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_max 91\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kor [['버지니아', '공대', '4', '학년', '학생', '인', '셰', '인', '무어', '는', '17', '일', '현지', '시간', '조', '와', '3', '년', '전', '점심', '을', '같이', '먹', '은', '적', '이', '있', '다고', '말', '했', '다', '.'], ['47484', '보여', '주', '어야', '한다고', '말', '했', '다', '.', '특히', '그', '는', '북한', '이', '슈퍼', '노트', '를', '제조', '할', '수', '있', '는', '장비', '와', '동판', '을', '제거', '했', '다는', '증거', '를', '제시', '해야', '앞', '으로', '지폐', '를', '제작', '할', '수', '있', '는', '능력', '이', '있', '다는', '염려', '가', '줄어들', '것', '이', '다', '라고', '말', '했', '다', '.'], ['최근', 'farc', '로부터', '석방', '된', '한', '피랍', '자', '는', '베탕쿠르', '의', '건강', '이', '매우', '악화', '됐', '다고', '진술', '했으며', '일부', '언론', '은', '그', '가', '피랍', '된', '지', '6', '주년', '이', '되', '는', '지난', '2', '월', '23', '일', '부터', '단식', '투쟁', '을', '시작', '했', '다고', '보도', '했', '다', '.']]\n",
      "eng [['<BOS>', 'virginia', 'tech', 'senior', 'shane', 'moore', 'said', 'tuesday', 'he', 'recalled', 'having', 'lunch', 'with', 'cho', 'three', 'years', 'ago', '.', '<EOS>'], ['<BOS>', 'specifically', ',', 'he', 'said', ',', 'the', 'north', 'must', 'provide', 'evidence', 'that', 'the', 'equipment', 'and', 'plates', 'for', 'the', 'so', 'called', 'supernotes', 'had', 'been', 'destroyed', 'so', 'that', 'concerns', 'about', 'further', 'ability', 'to', 'print', 'more', 'notes', 'will', 'be', 'reduced', '.', '<EOS>'], ['<BOS>', 'hostages', 'whom', 'the', 'rebel', 'group', 'recently', 'freed', 'said', 'she', 'is', 'in', 'poor', 'health', ',', 'and', 'other', 'reports', 'have', 'said', 'she', 'began', 'a', 'hunger', 'strike', 'february', '23', ',', 'the', 'sixth', 'anniversary', 'of', 'her', 'captivity', '.', 'so', ',', 'you', 'who', 'lead', 'farc', ',', 'you', 'now', 'have', 'a', 'date', 'with', 'history', ',', 'sarkozy', 'warned', '.', '<EOS>']]\n"
     ]
    }
   ],
   "source": [
    "## 문장길이별 분포 \n",
    "korlen = [len(x) for x in kor_corpus]\n",
    "kor_max= np.max(korlen)\n",
    "print(\"kor_max\", kor_max)\n",
    "plt.hist(korlen, bins= 100)\n",
    "plt.title(\"kor\")\n",
    "plt.show()\n",
    "\n",
    "englen = [len(x) for x in eng_corpus]\n",
    "eng_max= np.max(englen)\n",
    "print(\"eng_max\", eng_max)\n",
    "plt.hist(englen, bins= 100, color='red')\n",
    "plt.title(\"eng\")\n",
    "plt.show()\n",
    "\n",
    "## sample\n",
    "print(\"kor\",kor_corpus[:3])\n",
    "print(\"eng\",eng_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "940f40a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[['가족'], ['국어사전'], ['과일'], ['영어사전'], ['어휘'], ['영어사전'], ['영어사전'], ['번역'], ['번역'], ['영어사전'], ['국어사전'], ['어휘'], ['어휘'], ['한자사전'], ['산업자원부'], ['영어사전'], ['영어사전'], ['번역'], ['번역'], ['대상포진'], ['어휘'], ['영어사전'], ['번역'], ['영어사전'], ['장소'], ['한자사전'], ['어휘'], ['국어사전'], ['번역'], ['면접'], ['번역'], ['영어사전'], ['어휘'], ['번역'], ['영어사전'], ['원제']]\n"
     ]
    }
   ],
   "source": [
    "## 한단어이하 문장: 한단어짜리라도 번역에는 의미가 있으므로 사용 \n",
    "lowbins = [x for x in korlen if x <=1]\n",
    "print(lowbins)\n",
    "low_sen = [x for x in kor_corpus if len(x) <=1]\n",
    "print(low_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258753a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enc_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11d7dcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981d76a883d744dea6b86df7674d0d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰의 길이가 40미만으로 축소후 src_corpus 23406 [['버지니아', '공대', '4', '학년', '학생', '인', '셰', '인', '무어', '는', '17', '일', '현지', '시간', '조', '와', '3', '년', '전', '점심', '을', '같이', '먹', '은', '적', '이', '있', '다고', '말', '했', '다', '.'], ['폭스', '뉴스', '가', '입수', '한', '법원', '문서', '에', '의하', '면', '잭슨', '이', '경매', '등록', '일', '인', '내달', '19', '일', '까지', '빚', '을', '갚', '지', '않', '으면', '부동산', '이', '경매', '에', '부쳐진다', '.']] [['마이', '스페이스', '는', '지난주', ',', '중남미', '용', '마이', '스페이스', '운영', '을', '시작', '하', '면서', '세계', '로', '시장', '을', '확대', '해', '나가', '고', '있', '다', '.'], ['또한', '자국', '과', '일본', '주둔', '5', '만', '명', '의', '미군', '에', '대한', '방어', '목적', '으로', '도쿄', '는', '2', '개', '의', '미사일', '방어', '시스템', '을', '구입', '하', '기', '로', '결정', '했', '다', '.']]\n",
      "토큰의 길이가 45미만으로 축소후 tgt_corpus 23406 [['<BOS>', 'virginia', 'tech', 'senior', 'shane', 'moore', 'said', 'tuesday', 'he', 'recalled', 'having', 'lunch', 'with', 'cho', 'three', 'years', 'ago', '.', '<EOS>'], ['<BOS>', 'court', 'documents', 'obtained', 'by', 'fox', 'news', 'warn', 'jackson', 'that', 'he', 'has', 'until', 'the', 'date', 'of', 'the', 'auction', 'to', 'take', 'action', 'to', 'keep', 'his', 'lavish', 'los', 'olivos', 'estate', '.', '<EOS>']] [['<BOS>', 'last', 'week', ',', 'it', 'expanded', 'its', 'offerings', 'for', 'latinos', 'based', 'in', 'the', 'united', 'states', '.', '<EOS>'], ['<BOS>', 'tokyo', 'is', 'also', 'currently', 'in', 'the', 'process', 'of', 'buying', 'two', 'missile', 'defense', 'systems', ',', 'after', 'which', 'it', 'will', 'be', 'responsible', 'for', 'defending', 'not', 'only', 'itself', 'but', 'also', 'the', '50', ',', '000', 'us', 'troops', 'stationed', 'on', 'its', 'territory', '.', '<EOS>']]\n",
      "보정전 단어사전 길이: 29779 29779\n",
      "보정전 tokenizer.index_word[0~10] [(1, '<UNK>'), (2, '.'), (3, '다'), (4, '이'), (5, '는'), (6, '을'), (7, '의'), (8, '은'), (9, '에'), (10, '했')]\n",
      "tokenizer.index_word <PAD> <UNK> .\n",
      "tokenizer.word_index 0 3\n",
      "보정후 tokenizer.index_word[0~10] [(5, '.'), (6, '다'), (7, '이'), (8, '는'), (9, '을'), (10, '의'), (11, '은'), (12, '에'), (13, '했'), (14, '를'), (15, '하')]\n",
      "보정후 단어사전 길이: 29780 29780\n",
      "보정전 단어사전 길이: 26917 26917\n",
      "보정전 tokenizer.index_word[0~10] [(1, '<UNK>'), (2, 'the'), (3, '.'), (4, '<BOS>'), (5, '<EOS>'), (6, ','), (7, 'to'), (8, 'of'), (9, 'a'), (10, 'in'), (11, 'and')]\n",
      "tokenizer.index_word <PAD> <BOS> <EOS> <UNK> the\n",
      "tokenizer.word_index 0 1 2 3\n",
      "보정후 tokenizer.index_word[0~10] [(5, 'the'), (6, '.'), (9, ','), (10, 'to'), (0, '<PAD>'), (1, '<BOS>'), (2, '<EOS>'), (3, '<UNK>')]\n",
      "보정후 단어사전 길이: 26918 26918\n",
      "enc_train 23406 [[ 1226  2750    84  4755   368    29  5657    29  3468     8   498    28\n",
      "     71    57   515    32    67    41    55  5150     9   735  1073    11\n",
      "     35     7    18    26    34    13     6     5     0     0     0     0\n",
      "      0     0     0]\n",
      " [ 3156   547    17  2263    22   351  1780    12   740    90  2264     7\n",
      "   1574  1689    28    29  7012   380    28    79  3002     9 11912    33\n",
      "     45  1227  1781     7  1574    12 16380     5     0     0     0     0\n",
      "      0     0     0]] [[2862 3155    8  784   21 4146  744 2862 3155  576    9  140   15  175\n",
      "   118   25  210    9 1453   56 1203   16   18    6    5    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 164 1662   27  154  586   87   50   31   10  169   12   68 1988 1212\n",
      "    23 1426    8   54  122   10  657 1988  893    9 2300   15   39   25\n",
      "   214   13    6    5    0    0    0    0    0    0    0]]\n",
      "dec_train 23406 [[    1   986  1380   589 11401  3465    16    97    22  3636   691  4109\n",
      "     25  2079    89    83   275     6     2     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    1   181  1283  3637    29  2682   150  5555  2327    19    22    27\n",
      "    345     5  1234    11     5  2802    10   177   624    10   574    31\n",
      "   5087   832 15353  2163     6     2     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]] [[    1    68   110     9    23  5034    46  8691    17 26920   420    13\n",
      "      5    96   116     6     2     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    1   869    20    55   892    13     5   664    11  2340    50   814\n",
      "    359  1557     9    43    56    23    37    32  1226    17  3516    38\n",
      "    142  1573    36    55     5   355     9   100   123   172  3998    18\n",
      "     46  1846     6     2     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm    # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 40 이하(영어는 45)인 문장만 남김. \n",
    "for idx in tqdm(range(len(kor_corpus))):\n",
    "    if len(kor_corpus[idx]) < 40 and len(eng_corpus[idx]) < 45 :\n",
    "        src_corpus.append(kor_corpus[idx])\n",
    "        tgt_corpus.append(eng_corpus[idx])\n",
    "\n",
    "print(\"토큰의 길이가 40미만으로 축소후 src_corpus\", len(src_corpus), src_corpus[:2],src_corpus[-2:])\n",
    "print(\"토큰의 길이가 45미만으로 축소후 tgt_corpus\", len(tgt_corpus), tgt_corpus[:2],tgt_corpus[-2:])\n",
    "\n",
    "# 정수인덱스 토큰화및 패딩처리를 완료하여 학습용 데이터를 완성. \n",
    "enc_train, enc_tokenizer = tokenize(src_corpus, maxlen= 39, kor = True)\n",
    "dec_train, dec_tokenizer = tokenize(tgt_corpus, maxlen= 44, kor = False)\n",
    "\n",
    "print(\"enc_train\", len(enc_train), enc_train[:2],enc_train[-2:])\n",
    "print(\"dec_train\", len(dec_train), dec_train[:2],dec_train[-2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc4da3",
   "metadata": {},
   "source": [
    "#### train data, valid 분리 \n",
    "data 전체를 사용하고서, train data 와 valid data를 분리하면, 검증도 할수있고, train data 축소효과도 있어서, data 전체를 사용하고서, train, valid로 분리  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffd76366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc (18724, 39) (4682, 39)\n",
      "dec (18724, 44) (4682, 44)\n",
      "한글index_word길이: 29780\n",
      "영어index_word길이: 26918\n",
      "['러시아', '는', '지난', '29', '일', '신', '형', '다탄두', '대륙', '간', '탄도', '미사일', '을', '시험', '발사', '했으며', '고위', '장성', '들', '은', '이', 'icbm', '이', '향후', '40', '년', '간', '러시아', '의', '안보', '를', '책임질', '것', '이', '라고', '밝혔', '다', '.', '<PAD>']\n",
      "['<BOS>', 'russia', 'on', 'tuesday', 'test', 'fired', 'a', 'new', 'intercontinental', 'ballistic', 'missile', 'with', 'multiple', 'warheads', 'and', 'a', 'new', 'cruise', 'missiles', ',', 'which', 'russian', 'generals', 'say', 'are', 'sufficient', 'to', 'ensure', 'the', 'country', 's', 'security', 'for', 'the', 'next', '40', 'years', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "# 8:2 으로 분리\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(enc_train, dec_train, test_size=0.2)\n",
    "\n",
    "print(\"enc\", enc_train.shape,enc_val.shape)\n",
    "print(\"dec\", dec_train.shape, dec_val.shape)\n",
    "\n",
    "print(\"한글index_word길이:\", len(enc_tokenizer.index_word))\n",
    "print(\"영어index_word길이:\", len(dec_tokenizer.index_word))\n",
    "\n",
    "print([enc_tokenizer.index_word[i] for i in enc_train[5]])\n",
    "print([dec_tokenizer.index_word[i] for i in dec_train[5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d2a0e4",
   "metadata": {},
   "source": [
    "#### Step 4. 모델 설계\n",
    "한국어를 영어로 잘 번역해 줄 멋진 Attention 기반 Seq2seq 모델을 설계하세요! 앞서 만든 모델에 Dropout 모듈을 추가하면 성능이 더 좋아집니다! Embedding Size와 Hidden Size는 실험을 통해 적당한 값을 맞춰 주도록 합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e8f0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        #print(\"attn dense h_enc.shape\",h_enc.shape)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        #print(\"attn  expand h_dec.shape\",h_dec.shape)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "        #print(\"attn dense h_dec.shape\",h_dec.shape)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        #print(\"attn combine tanh score.shape\",score.shape)\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "        #print(\"attn softmax_score.shape\",attn.shape)\n",
    "        context_vec = attn * h_enc\n",
    "        #print(\"attn attn * h_enc==context_vec.shape\",context_vec.shape)\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "        #print(\"attn reduce_sum axis=1 context_vec.shape\",context_vec.shape)\n",
    "        \n",
    "        return context_vec, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c74dff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, drate):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #, mask_zero=True)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units, return_sequences = True, recurrent_initializer='glorot_uniform')\n",
    "        self.batchnormal = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout = tf.keras.layers.Dropout(drate)\n",
    "        \n",
    "    def call(self, x):\n",
    "       \n",
    "        out = self.embedding(x)\n",
    "        #out = self.batchnormal(out)\n",
    "        #out = self.dropout(out)\n",
    "        out = self.gru(out)\n",
    "        #out = self.batchnormal(out)\n",
    "        #out = self.dropout(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21cbd202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, drate):\n",
    "        super(Decoder, self).__init__()\n",
    "       \n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #, mask_zero=True)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)   # Attention 필수 사용!\n",
    "\n",
    "        self.gru = tf.keras.layers.GRU(dec_units, return_sequences = True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.batchnormal = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout = tf.keras.layers.Dropout(drate)\n",
    "        \n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        #print(\"디코더내 input x.shape\",x.shape)\n",
    "       \n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "        #print(\"디코더내 context_vec.shape\",context_vec.shape, \"attn.shape\", attn.shape)\n",
    "        out = self.embedding(x)\n",
    "        #print(\"디코더내 emb의 output out.shape\",out.shape)\n",
    "        #out = self.batchnormal(out)\n",
    "        #out = self.dropout(out)\n",
    "        #print(\"디코더내 decoder dropout embedding xoutshape\",out.shape)\n",
    "       \n",
    "        out = tf.concat([tf.expand_dims(context_vec,1), out], axis = -1)\n",
    "        #print(\"디코더내 tf.concat out@@@\", out)\n",
    "        out, h_dec = self.gru(out)\n",
    "        #out = self.batchnormal(out)\n",
    "        #out = self.dropout(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        #print(\"디코더함수내_ pred\", out.shape,out)\n",
    "        \n",
    "        return out, h_dec, attn\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f4eb436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n",
      "Encoder Output: (64, 39, 1024)\n",
      "sample_state: (64, 1024)\n",
      "Decoder Output: (64, 10000) tf.Tensor(\n",
      "[[ 8.5985445e-04  1.2641987e-03 -6.4869778e-07 ...  3.4447538e-04\n",
      "   5.4878241e-04  1.2374237e-03]], shape=(1, 10000), dtype=float32)\n",
      "Decoder Hidden State: (64, 1024) tf.Tensor(\n",
      "[[ 0.00709115  0.00173957  0.0096751  ...  0.01364668  0.01383762\n",
      "  -0.01832651]], shape=(1, 1024), dtype=float32)\n",
      "Attention: (64, 39, 1)\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행하세요.\n",
    "\n",
    "BATCH_SIZE     =  64  #6432  #512 #64 \n",
    "SRC_VOCAB_SIZE = 10000 #20000       #len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = 10000 # 20000       #len(dec_tokenizer.index_word) + 1\n",
    "print(SRC_VOCAB_SIZE,TGT_VOCAB_SIZE)\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "drate = 0.1\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units, drate)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units, drate)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 39\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "print ('sample_state:', sample_state.shape)\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape, sample_logits[:1])\n",
    "print ('Decoder Hidden State:', h_dec.shape, h_dec[:1])\n",
    "print ('Attention:', attn.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb83e602",
   "metadata": {},
   "source": [
    "#### Step 5. 훈련하기\n",
    "훈련엔 위에서 사용한 코드를 그대로 사용하되, eval_step() 부분이 없음에 유의합니다! 매 스텝 아래의 예문에 대한 번역을 생성하여 본인이 생각하기에 가장 멋지게 번역한 Case를 제출하세요! (Attention Map을 시각화해보는 것도 재밌을 거예요!)\n",
    "\n",
    "❕ 참고: 데이터의 난이도가 높은 편이므로 생각만큼 결과가 잘 안나올 수 있습니다.\n",
    "```\n",
    "## 예문 ##\n",
    "K1) 오바마는 대통령이다.\n",
    "K2) 시민들은 도시 속에 산다.\n",
    "K3) 커피는 필요 없다.\n",
    "K4) 일곱 명의 사망자가 발생했다.\n",
    "\n",
    "## 제출 ##\n",
    "E1) obama is the president . <end>\n",
    "E2) people are victims of the city . <end>\n",
    "E2) the price is not enough . <end>\n",
    "E2) seven people have died . <end>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7987bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()  #learning_rate=0.001,\n",
    "#optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001) \n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "#loss_object = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e83f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "    \n",
    "    #print(\"train_step f>> src\",src.shape)\n",
    "    #print(\"train_step f>> tgt\",tgt.shape)\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        #print(\"train_step f>>enc_out\", enc_out.shape)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        #print(\"train_step f>> h_dec @@@@@@\", h_dec.shape) #, h_dec[:3])\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<BOS>']] * bsz, 1)  # dec_input= (64,1)\n",
    "        #print(\"dec_src@@@\",dec_src.shape, dec_src)\n",
    "        #[주의!!] <start>,<end>처리의 두 방식(for loop vs while):   for loop 를 1부터시작해서,tgt의 <start>제외함, while로 할 경우 <end>나 maxlen에서 멈추게하는데,\n",
    "        #==> 여기서는 for loop로 처리해서,<end>유무상관없이 maxlen 문장끝에서 멈춤(<end>다음순서에 padding존재,연산이 많아지진 않나??)\n",
    "        #==> 이러한 for loop 방식을 사용해서, 이 프로젝트에서는 dec_src는 <end>제거, dec_target에는 <start>제거 코드가 없슴\n",
    "         \n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "            #print(\"pred\",pred.shape, pred)\n",
    "            #print(\"t\",t, \"tgt[:, t]\", tgt[:, t], \"tgt[:, t].shape\",tgt[:, t].shape,\"@@@\",\"tf.expand_dims(tgt[:, t],axis=1)\",tf.expand_dims(tgt[:, t],axis=1))\n",
    "            \n",
    "            loss += loss_function(tgt[:, t], pred) #원본\n",
    "            #loss += loss_function(tf.expand_dims(tgt[:, t],axis=1), pred) #?????????????????????\n",
    "            \n",
    "            #print(\"loss\", loss.shape, loss, \"tgt.shape\", tgt.shape)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)  # teacher forcing: tgt[:, t]==> real label==> <end>는 padding직전에 존재하게됨 \n",
    "            #print(\"dec_src ***\",dec_src.shape)\n",
    "            \n",
    "    batch_loss = (loss / int(tgt.shape[1]))  # 문장별 총단어토큰별 loss임, batch_size로 나누지 않고, 토큰수로 나눔, batch_size로 나누는 것은\n",
    "    ##==> 아래 학습 loop 코드내에서, epock 돌때, batch 1개가 끝날때 마다, batch size로 나누어서, 프린트해서 보여줌 !!!!!!!!!!!!!!!!\n",
    "    #batch_loss = (loss / int(tgt.shape[0])) #my  ==> 여기선 이 코드는 틀림\n",
    "    \n",
    "    #print(\"batch_loss\", batch_loss.shape, batch_loss)\n",
    "    \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19e507b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint_dir = 'training_checkpoints'\n",
    "#checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "#checkpoint = tf.train.Checkpoint(optimizer=optimizer,  encoder=encoder,  decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a72c24c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 293/293 [03:13<00:00,  1.51it/s, Loss nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch당_소요시간 3.2287479201952616 분\n",
      "현재까지_누적_소요시간 3.228750228881836 분\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2: 100%|██████████| 293/293 [02:01<00:00,  2.42it/s, Loss nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch당_소요시간 2.0194777051607766 분\n",
      "현재까지_누적_소요시간 5.248229972521464 분\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 293/293 [02:01<00:00,  2.42it/s, Loss nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch당_소요시간 2.017473614215851 분\n",
      "현재까지_누적_소요시간 7.265706904729208 분\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4: 100%|██████████| 293/293 [02:01<00:00,  2.42it/s, Loss nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch당_소요시간 2.0180379152297974 분\n",
      "현재까지_누적_소요시간 9.283748575051625 분\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5: 100%|██████████| 293/293 [02:01<00:00,  2.42it/s, Loss nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch당_소요시간 2.0185569723447165 분\n",
      "현재까지_누적_소요시간 11.302308905124665 분\n",
      "최종소요시간 11.302325475215913 분\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm    # tqdm\n",
    "import random, time\n",
    "\n",
    "EPOCHS = 5 #10\n",
    "stime = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_stime = time.time()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)    # tqdm\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        #print(\"enc_train[idx:idx+BATCH_SIZE]@@@@\",enc_train.shape, enc_train[idx:idx+BATCH_SIZE])\n",
    "        #print(\"dec_train[idx:idx+BATCH_SIZE]@@@@\",dec_train.shape, dec_train[idx:idx+BATCH_SIZE])\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        #print(\"batch_loss!!!!!!!!!!!\", batch_loss, total_loss, total_loss.numpy())\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))    # tqdm\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))    # tqdm\n",
    "        \n",
    "    epoch_etime = time.time()\n",
    "    print(\"Epoch당_소요시간\", (epoch_etime-epoch_stime)/ 60, \"분\")\n",
    "    print(\"현재까지_누적_소요시간\", (epoch_etime-stime)/ 60, \"분\")\n",
    "    \n",
    "etime = time.time()\n",
    "print(\"최종소요시간\", (etime-stime)/ 60, \"분\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbcad7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5284ceb",
   "metadata": {},
   "source": [
    "#### 인코더,디코더 저장: 서브클래싱모델은 HDF5 format으로 저장않됨,케창투2 참조필요, 또는 tf save_weight, 또는 현재코드로 저장해서 나오는 오류볼것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2df3ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "save_path  = '/aiffel/aiffel/s2s_translation/model'\n",
    "\n",
    "#encoder.save(save_path + '/s2s_encoder_model.h5')\n",
    "#decoder.save(save_path + '/s2s_decoder_model.h5')\n",
    "\n",
    "#encoder = load_model('s2s_encoder_model.h5')\n",
    "#decoder = load_model('s2s_decoder_model.h5')\n",
    "\n",
    "#model_to_dot(encoder, show_shapes=True)\n",
    "#model_to_dot(decoder, show_shapes=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c4024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cb28883",
   "metadata": {},
   "source": [
    "#### S2S_Attention 번역모델  성능테스트 함수정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ba5875cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention.shape (44, 39)\n",
      "전처리_sentence 커피 한잔부탁해요 .\n",
      "정수토큰_inputs1_enc_tokenize  [[1901, 3, 5]]\n",
      "패드_inputs2_pad [[1901    3    5    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n",
      "enc_out (1, 39, 1024) tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(1, 39, 1024), dtype=float32)\n",
      "dec_hidden (1, 1024) tf.Tensor([[nan nan nan ... nan nan nan]], shape=(1, 1024), dtype=float32)\n",
      "loop직전 dec_input tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "Input: 커피 한잔부탁해요 .\n",
      "Predicted translation: <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134/2670381119.py:61: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "/tmp/ipykernel_134/2670381119.py:62: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    print(\"attention.shape\",attention.shape)\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    print(\"전처리_sentence\", sentence)\n",
    "        \n",
    "    # 정수 토큰화: 별도로 만든 객체로 해야 정수인덱스 짝이 맞음 \n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])   # 별도객체아닌 원문\n",
    "    print(\"정수토큰_inputs1_enc_tokenize \",inputs,)\n",
    "    \n",
    "    #sentence_word = ' '.join([[enc_tokenizer.index_word[idx] for idx in seq] for seq in inputs][0])\n",
    "    #print(\"sentence_word\", sentence_word)\n",
    "    \n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=enc_train.shape[-1],padding='post')\n",
    "    print(\"패드_inputs2_pad\",inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "    print(\"enc_out\",enc_out.shape, enc_out[:2])\n",
    "    \n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    print(\"dec_hidden\",dec_hidden.shape, dec_hidden[:2])\n",
    "    \n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<BOS>']], 0)\n",
    "    print(\"loop직전 dec_input\", dec_input)\n",
    "    \n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        #print(\"t\",t)\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        #print(\"predictions\", predictions)\n",
    "        #print(\"U attention_weights\", attention_weights)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        #print(\"D attention_weights\", attention_weights)\n",
    "        attention[t] = attention_weights.numpy()\n",
    "        #print(\"attention[t]\",attention[t])\n",
    "        \n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "        #print(\"predicted_id\",predicted_id)\n",
    "        \n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "        #print(\"result\", result)\n",
    "        \n",
    "        if dec_tokenizer.index_word[predicted_id] == '<EOS>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        #print(\"dec_input\",dec_input)\n",
    "        \n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder, plot= False ):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    if plot==True:\n",
    "        plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "\n",
    "translate(\"커피 한잔부탁해요.\", encoder, decoder, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ba6d6",
   "metadata": {},
   "source": [
    "#### S2S_Attention 번역모델  성능테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "be37bfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 문장: 오바마는 대통령이다.\n",
      "attention.shape (44, 39)\n",
      "전처리_sentence 오바마는 대통령이다 .\n",
      "정수토큰_inputs1_enc_tokenize  [[3, 3, 5]]\n",
      "패드_inputs2_pad [[3 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0]]\n",
      "enc_out (1, 39, 1024) tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(1, 39, 1024), dtype=float32)\n",
      "dec_hidden (1, 1024) tf.Tensor([[nan nan nan ... nan nan nan]], shape=(1, 1024), dtype=float32)\n",
      "loop직전 dec_input tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "return 원래 문장: 오바마는 대통령이다 .\n",
      "번역문장 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "원래 문장: 시민들은 도시 속에 산다.\n",
      "attention.shape (44, 39)\n",
      "전처리_sentence 시민들은 도시 속에 산다 .\n",
      "정수토큰_inputs1_enc_tokenize  [[3, 325, 3, 3, 5]]\n",
      "패드_inputs2_pad [[  3 325   3   3   5   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]]\n",
      "enc_out (1, 39, 1024) tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(1, 39, 1024), dtype=float32)\n",
      "dec_hidden (1, 1024) tf.Tensor([[nan nan nan ... nan nan nan]], shape=(1, 1024), dtype=float32)\n",
      "loop직전 dec_input tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "return 원래 문장: 시민들은 도시 속에 산다 .\n",
      "번역문장 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "원래 문장: 커피는 필요 없다.\n",
      "attention.shape (44, 39)\n",
      "전처리_sentence 커피는 필요 없다 .\n",
      "정수토큰_inputs1_enc_tokenize  [[3, 289, 1080, 5]]\n",
      "패드_inputs2_pad [[   3  289 1080    5    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n",
      "enc_out (1, 39, 1024) tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(1, 39, 1024), dtype=float32)\n",
      "dec_hidden (1, 1024) tf.Tensor([[nan nan nan ... nan nan nan]], shape=(1, 1024), dtype=float32)\n",
      "loop직전 dec_input tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "return 원래 문장: 커피는 필요 없다 .\n",
      "번역문장 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "원래 문장: 일곱 명의 사망자가 발생했다.\n",
      "attention.shape (44, 39)\n",
      "전처리_sentence 일곱 명의 사망자가 발생했다 .\n",
      "정수토큰_inputs1_enc_tokenize  [[4838, 28183, 3, 3, 5]]\n",
      "패드_inputs2_pad [[ 4838 28183     3     3     5     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]]\n",
      "enc_out (1, 39, 1024) tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(1, 39, 1024), dtype=float32)\n",
      "dec_hidden (1, 1024) tf.Tensor([[nan nan nan ... nan nan nan]], shape=(1, 1024), dtype=float32)\n",
      "loop직전 dec_input tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "return 원래 문장: 일곱 명의 사망자가 발생했다 .\n",
      "번역문장 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "translate_list ['<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> ', '<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> ', '<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> ', '<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> ']\n"
     ]
    }
   ],
   "source": [
    "sentence_list = ['오바마는 대통령이다.', '시민들은 도시 속에 산다.', '커피는 필요 없다.', '일곱 명의 사망자가 발생했다.']\n",
    "translate_list = []\n",
    "for sentence in sentence_list:\n",
    "    print(\"원래 문장:\", sentence)\n",
    "    trans_result,sentence, attension_weight = evaluate(sentence, encoder, decoder) #translate_sequence(sentence, encoder, decoder)\n",
    "    print(\"return 원래 문장:\", sentence)\n",
    "    print(\"번역문장\",trans_result)\n",
    "    translate_list.append(trans_result)  \n",
    "    \n",
    "print(\"translate_list\",translate_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850714b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70a8bbde",
   "metadata": {},
   "source": [
    "## 회고 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49497e4",
   "metadata": {},
   "source": [
    "본 프로젝트는 seq2seq with attention으로 한글을 영어로 번역하는 번역기를 만들어 보는 프로젝트입니다.\n",
    "\n",
    "한글및 영어데이터를 데이터 전처리 및 한글 형태소분석, 정수인덱스 토크나이징 처리, 보정처리 등을 거의 오차없이 제대로 처리했습니다.\n",
    "\n",
    "모델도 LMS 교재대로 해서, 한글을 영어로 번역하는 작업이 잘되고 있다고 생각했는데, 학습이 될때, loss function에서, Nan값이 계속 발생하며, 학습이 않되어서, 데이터 작업을 여러번 혹 실수가 있었나 찾아보고, 인코더및 디코더, 어텐션등 모델의 제반 내외부를 분리하여, 인풋 아웃풋을 체크하며, shape를 체쿠해봐도  틀리는 곳이 없는데도 불구하고, 지속적으로 loss function에 Nan이 발생되는데다, 제출시한이 다되어,\n",
    "전체 작업단계대로 제출본을 마무리해서 제출하게 되었습니다.\n",
    "\n",
    "중요한 seq2seq with attention과 transformer의  첫번째 프로젝트에서 시한이 임박해서, 해결을 못하고 제출하게 되었습니다.맥이 좀 풀리지만, seq2seq with attention과 transformer는 마스터하려고 마음을 먹고 있었기 때문에, 진도맞추어 나가면서, 필히 해결할 생각입니다.\n",
    "\n",
    "분명 단순하지만 놓치기쉬운 부분의 꼬임에서 발생된 것일테니, 시간깆고서,하나 하나 뜯어보면 쉽게 해결될수도 있을 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5423d699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
