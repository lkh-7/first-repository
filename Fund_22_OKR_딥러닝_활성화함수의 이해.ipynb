{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 22-2. 활성화 함수\n",
    "\n",
    "- 비선형성 이라는 키워드를 검색해보고, 비선형 함수라고 말할 수 있는 조건을 이해한대로 설명해보았다. \n",
    "\n",
    "===> 선형성의 조건:수학적으로가 아닌 직관적으로 설명하면,  x,y 좌표상에서, 벡터들의 덧셈, 곱셈등을 통하여, 벡터들에 변화를 줄때, \n",
    "        그 변화된 벡터가 직선을 유지하며, 간격이 균등하고, 원점인 (0,0)을 벗어나지 않게 변화를 줄때, 선형변환이라 함 \n",
    "===> 비선형은 위에서 정의한 선형성의 조건을 충족하지 않은 것을 비선형이라함\n",
    "\n",
    "- 비선형 데이터를 표현하려면 딥러닝 모델도 비선형성을 지니고 있어야 한다.\n",
    "\n",
    "===> 비선형데이터일경우 당연히 비선형모델을 써야할것 같다\n",
    "\n",
    "- 데이터가 비선형성을 띄고 있는 사례를 찾아보고 서로에게 소개해보자.\n",
    "\n",
    "===> 트리와 같은 계층적데이터의 경우 비선형성을 띔\n",
    "\n",
    "- 비선형 활성화함수를 사용해 선형회귀식을 합성했을 때 실제로 비선형 모델이 만들어지는지 실습해보자.\n",
    "\n",
    "===> \n",
    "\n",
    "-두 함수 f(x)와 g(x)를 중첩시킨 h(x) 함수가 어떤 형태인지 그래프를 잼보드에 직접 그려보고 서로에게 설명해보자.\n",
    "\n",
    "===> y= h(f(g(x))), 딥러닝모델의 레이어와같이, 인풋,가중치,활성화함수,레이어의 output의 경우가 중첩함수임\n",
    "\n",
    "-10 <= x <= 10 일때, f(x) = ReLU(2 * x), g(x) = ReLU(1 - x) 함수 f와 g를 합성한 합성함수 h(x) = g(f(x)) 를 그려봅시다\"\n",
    "\n",
    "===>\n",
    "\n",
    "- 한 명씩 돌아가며 Sigmoid, Tanh, ReLU 의 그래프를 최대한 정확하게 직접 그려보고, \n",
    "함수 별 특성을 각각 설명해보았다. \n",
    "(가로-세로 축, 최대-최소값, y절편, 도함수=미분)\"\n",
    "\n",
    "===>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55885303",
   "metadata": {},
   "outputs": [],
   "source": [
    "22-9 비선형 활성화 함수-시그모이드, Softmax\n",
    "\n",
    "\" - 시그모이드 함수의 첫번째 단점을 설명한 문단입니다\n",
    "\"\"극단적인 예로 만약 어떤 모델의 초기 가중치 값들을 아주 크게 잡아 포화상태를 만들면\n",
    "역전파 때 그래디언트가 죽기 때문에 아무리 많이 에포크를 돌려도 훈련이 거의 되지 않습니다.”\n",
    "시그모이드 함수의 미분 공식을 적용해 왜 역전파시 가중치 업데이트가 불가능해질 수 있는지 설명해보자.\"\n",
    "\n",
    "===> 시그모이드함수의 도함수는 sigmoid(1-sigmoid)로서, 미분값의 최대치는 0.5(1-05) == 0.25임, \n",
    "      미분값이 소수점인데, 최대값이 0.25 다보니, layer가 여러층일때, 소수점끼리 곱해져서, 기울기가 거의 0에 가깝게되며,\n",
    "      가중치 갱신이 않됨, (vanishing gradient)\n",
    "\n",
    "\n",
    "\" - 시그모이드 함수의 미분 공식을 통해 어떤 노드의 입력값이 모두 양수일 경우, \n",
    "해당 노드의 가중치들이 모두 양수로 혹은 모두 음수로만 업데이트 될 가능성이 왜 존재하는지 이야기해보고,\n",
    "이것이 학습에 미치는 부정적인 영향이 무엇인지 서로에게 설명해보자.\"\n",
    "     \n",
    "===> 시그모이드함수의 미분값이 양수로만 나오기때문에 즉, sigmoid*(sigmoid-1)인데, 0 <sigmoid <1 이므로, 미분시 항상 양의 값이라서,\n",
    "    upstream gradient가 곱해질때, upstream gradient의 부호대로 모든 가중치가 갱신될수 밖에 없다, 이것도 vanishing gradient와 함께,\n",
    "    시그모임드함수의 단점임\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "22-10 비선형 활성화 함수-하이퍼볼릭 탄젠트\n",
    " - 하이퍼볼릭 탄젠트 함수에서 출력값이 -1과 1에 포화되는 것이 어떤 문제를 불러 일으킬 수 있을지 서로에게 설명해보자\n",
    "    \n",
    "===> tanh도 -1에서  1 사이에서 움직이는데, 결국은 소수점이라, 층이 깊어지면 소수점끼리의 곱셈이라서, 가중치가 소실될수 있슴     \n",
    "    \n",
    "    ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e4a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "22-11 신경망의 너비변화가 주는 효과\n",
    "\" - 세번째 코드블럭에서 노드를 병렬로 쌓은 모델 approx_relu_model_p 과\n",
    "노드를 직렬로 쌓은 모델 approx_relu_model_s 가 각각 어떤 의미이고, \n",
    "모델 출력결과에 있어 둘에 어떤 차이가 있는건지 토론해보자.\"\n",
    "\n",
    "===> 병렬은 한개의 레이어내에 유닛(노드)수가 많아지게 설계한 것이고,\n",
    "      직렬은, 한개레이어별 유닛(노드)수는 적지만, 층을 깊게 하는 것을 의미\n",
    "     병렬과 직렬의 파라메터수가 같게되도록 두 모델을 설계했을 경우, \n",
    "        병렬이 더 성능이 좋은 손실함수가 있고, 직렬이 성능이 더 좋은 함수가 있다.\n",
    "     y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554c41d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485c1034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f54e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b887c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "22-2. 활성화 함수\n",
    " - 비선형성 이라는 키워드를 검색해보고, 비선형 함수라고 말할 수 있는 조건을 이해한대로 설명해보았다. \n",
    "\" - \"\"비선형 데이터를 표현하려면 딥러닝 모델도 비선형성을 지니고 있어야 한다.”\n",
    "데이터가 비선형성을 띄고 있는 사례를 찾아보고 서로에게 소개해보자.\"\n",
    "\" - 비선형 활성화함수를 사용해 선형회귀식을 합성했을 때 실제로 비선형 모델이 만들어지는지 실습해보자.\n",
    "두 함수 f(x)와 g(x)를 중첩시킨 h(x) 함수가 어떤 형태인지 그래프를 잼보드에 직접 그려보고 서로에게 설명해보자.\n",
    "-10 <= x <= 10 일때, \n",
    "f(x) = ReLU(2 * x), g(x) = ReLU(1 - x) 함수 f와 g를 합성한 합성함수 h(x) = g(f(x)) 를 그려봅시다\"\n",
    "\" - 한 명씩 돌아가며 Sigmoid, Tanh, ReLU 의 그래프를 최대한 정확하게 직접 그려보고, \n",
    "함수 별 특성을 각각 설명해보았다. \n",
    "(가로-세로 축, 최대-최소값, y절편, 도함수=미분)\"\n",
    "\n",
    "\n",
    "22-9 비선형 활성화 함수-시그모이드, Softmax\n",
    "\" - 시그모이드 함수의 첫번째 단점을 설명한 문단입니다\n",
    "\"\"극단적인 예로 만약 어떤 모델의 초기 가중치 값들을 아주 크게 잡아 포화상태를 만들면\n",
    "역전파 때 그래디언트가 죽기 때문에 아무리 많이 에포크를 돌려도 훈련이 거의 되지 않습니다.”\n",
    "시그모이드 함수의 미분 공식을 적용해 왜 역전파시 가중치 업데이트가 불가능해질 수 있는지 설명해보자.\"\n",
    "\" - 시그모이드 함수의 미분 공식을 통해 어떤 노드의 입력값이 모두 양수일 경우, \n",
    "해당 노드의 가중치들이 모두 양수로 혹은 모두 음수로만 업데이트 될 가능성이 왜 존재하는지 이야기해보고,\n",
    "이것이 학습에 미치는 부정적인 영향이 무엇인지 서로에게 설명해보자.\"\n",
    "22-10 비선형 활성화 함수-하이퍼볼릭 탄젠트\n",
    " - 하이퍼볼릭 탄젠트 함수에서 출력값이 -1과 1에 포화되는 것이 어떤 문제를 불러 일으킬 수 있을지 서로에게 설명해보자.\n",
    "22-11 신경망의 너비변화가 주는 효과\n",
    "\" - 세번째 코드블럭에서 노드를 병렬로 쌓은 모델 approx_relu_model_p 과\n",
    "노드를 직렬로 쌓은 모델 approx_relu_model_s 가 각각 어떤 의미이고, \n",
    "모델 출력결과에 있어 둘에 어떤 차이가 있는건지 토론해보자.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
